{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Local Transformers_and_MHAttention.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "m9Mv5avjzPDe",
        "Anh5s5PfzPDg",
        "AfaSxGO_zPDh"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "affa01b2a0d24a379b741dbd31a106d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_64beaa33af10450bb793d23f5060693e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_eccb9582be6a4be2aa8620a8bdffb30c",
              "IPY_MODEL_6d7fc6c6a3fb4081a415810502e086da"
            ]
          }
        },
        "64beaa33af10450bb793d23f5060693e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "eccb9582be6a4be2aa8620a8bdffb30c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_51ebe70eda414907b93dd078364999d7",
            "_dom_classes": [],
            "description": "Validation sanity check: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f50a69a294e648259489265faa626e3a"
          }
        },
        "6d7fc6c6a3fb4081a415810502e086da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c1a0faac24574813895fab59a847b928",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2/2 [00:00&lt;00:00, 33.10it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_64e2b56e1cb4498087eab282b124bf5b"
          }
        },
        "51ebe70eda414907b93dd078364999d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f50a69a294e648259489265faa626e3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c1a0faac24574813895fab59a847b928": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "64e2b56e1cb4498087eab282b124bf5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6ec68fb804264a4c8f329439ce8c82fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_feddd3cebeec4343af71b038a1e920f5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a03a85a305334f938f2af794db87d5b7",
              "IPY_MODEL_e0c2826108cf410295b1c4cd861f0d0a"
            ]
          }
        },
        "feddd3cebeec4343af71b038a1e920f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "a03a85a305334f938f2af794db87d5b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d2472433e0474315b5701eec6117b392",
            "_dom_classes": [],
            "description": "Epoch 0:  83%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 398,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 329,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_428bd73bf32f4714862a0ddd5053ce81"
          }
        },
        "e0c2826108cf410295b1c4cd861f0d0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8c39933691eb4d55987256ad3defadb1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 329/398 [00:30&lt;00:06, 10.75it/s, loss=2.83, v_num=3]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_347a6df9d6b9420cb45f9e8cc02dabfc"
          }
        },
        "d2472433e0474315b5701eec6117b392": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "428bd73bf32f4714862a0ddd5053ce81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8c39933691eb4d55987256ad3defadb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "347a6df9d6b9420cb45f9e8cc02dabfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bd2958090d6b40b981aea2548a3cb953": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_14e602f71f774718b70c7a2dde4f05f0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_578900e4887b49f9a5e3e51cd25921b9",
              "IPY_MODEL_9d428ed928e04cfa8bf74925b82f8fee"
            ]
          }
        },
        "14e602f71f774718b70c7a2dde4f05f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "578900e4887b49f9a5e3e51cd25921b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_016cc48a766d4d8593d5de28824d9280",
            "_dom_classes": [],
            "description": "Testing: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e029382a284e4f92b65203936d58e08c"
          }
        },
        "9d428ed928e04cfa8bf74925b82f8fee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e641c4b20321405fb7b494cd8d08ddf0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 8/8 [18:18&lt;00:00, 137.32s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9dc194d6037e4962806f29f3cc41a9bf"
          }
        },
        "016cc48a766d4d8593d5de28824d9280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e029382a284e4f92b65203936d58e08c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e641c4b20321405fb7b494cd8d08ddf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9dc194d6037e4962806f29f3cc41a9bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1e00aad8be3541c28d41ef47607ffbe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0fd50de47c62433784d087708228f074",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e3c757273ff34cb19e05f3544b891260",
              "IPY_MODEL_bedda42b213446d69a73f593f064dd3a"
            ]
          }
        },
        "0fd50de47c62433784d087708228f074": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "e3c757273ff34cb19e05f3544b891260": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_94a43a8c83d94d9b9f709db3bff3e199",
            "_dom_classes": [],
            "description": "Testing: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9e9bb24b8a334ef7b8c6a02cb5a50aa1"
          }
        },
        "bedda42b213446d69a73f593f064dd3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2efbaff77a354fae91c73e7eb6a9005c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 79/79 [00:01&lt;00:00, 53.46it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d61932e5633147fe8a087770872731ff"
          }
        },
        "94a43a8c83d94d9b9f709db3bff3e199": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9e9bb24b8a334ef7b8c6a02cb5a50aa1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2efbaff77a354fae91c73e7eb6a9005c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d61932e5633147fe8a087770872731ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a6e70f11baa54e3da61ba4c3701bda21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_50941ae1b92f4f96a34d560f0a74af5b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9bc60ab01ff845ae9bef159a89b3cbce",
              "IPY_MODEL_f7b8f7d424a445fb981d4672144c5bbb"
            ]
          }
        },
        "50941ae1b92f4f96a34d560f0a74af5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "9bc60ab01ff845ae9bef159a89b3cbce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6cb09a617367447f9748f1ae20193544",
            "_dom_classes": [],
            "description": "Validation sanity check:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_724a8c5f72fe4890a9e1b31cff867122"
          }
        },
        "f7b8f7d424a445fb981d4672144c5bbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fadc8f933dbb468f9d0e72d8ddda167f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/2 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e4f2ea2f75a04e05849e30b5726c231e"
          }
        },
        "6cb09a617367447f9748f1ae20193544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "724a8c5f72fe4890a9e1b31cff867122": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fadc8f933dbb468f9d0e72d8ddda167f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e4f2ea2f75a04e05849e30b5726c231e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TzurV/Transformers/blob/main/NumbersGames_Transformers_and_MHAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDY2t72QzPDS"
      },
      "source": [
        "# Tzur Vaich : Transformers and Multi-Head Attention as Language Model\n",
        "\n",
        "![Status](https://img.shields.io/static/v1.svg?label=Status&message=WorkInProgress&color=red)\n",
        "\n",
        "\n",
        "**Filled notebook:** \n",
        "[![View on Github](https://img.shields.io/static/v1.svg?logo=github&label=Repo&message=View%20On%20Github&color=lightgrey)](https://github.com/TzurV/Transformers/blob/main/My_LM_Transformers_and_MHAttention.ipynb)\n",
        "[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/TzurV/Transformers/blob/main/My_LM_Transformers_and_MHAttention.ipynb)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M936pB7XzPDV"
      },
      "source": [
        "In this tutorial, we will discuss one of the most impactful architectures of the last 2 years: the Transformer model. Since the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. had been published in 2017, the Transformer architecture has continued to beat benchmarks in many domains, most importantly in Natural Language Processing. Transformers with an incredible amount of parameters can generate long, convincing [essays](https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3), and opened up new application fields of AI. As the hype of the Transformer architecture seems not to come to an end in the next years, it is important to understand how it works, and have implemented it yourself, which we will do in this notebook.\n",
        "\n",
        "Despite the huge success of Transformers in NLP, we will _not_ include the NLP domain in our notebook here. Why? Firstly, the Master AI at UvA offers many great NLP courses that will take a closer look at the application of the Transformer architecture in NLP ([NLP2](https://studiegids.uva.nl/xmlpages/page/2020-2021/zoek-vak/vak/79628), [Advanced Topics in Computational Semantics](https://studiegids.uva.nl/xmlpages/page/2020-2021/zoek-vak/vak/80162)). Secondly, assignment 2 takes already a closer look at language generation on character level, on which you could easily apply our transformer architecture. Finally, and most importantly, there is so much more to the Transformer architecture. NLP is the domain the Transformer architecture has been originally proposed for and had the greatest impact on, but it also accelerated research in other domains, recently even [Computer Vision](https://arxiv.org/abs/2010.11929). Thus, we focus here on what makes the Transformer and self-attention so powerful in general.\n",
        "\n",
        "Below, we import our standard libraries. As in Tutorial 5, we will use [PyTorch Lightning](https://www.pytorchlightning.ai/) as an additional framework. If you are not familiar with PyTorch Lightning, please make sure to have read Tutorial 5 carefully."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fcno0EjRXzzL"
      },
      "source": [
        "# Run This"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIN6mEKlzPDW",
        "outputId": "51dc3f09-c21a-4e9f-d7a2-0cdf5d65ec93"
      },
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import numpy as np \n",
        "import random\n",
        "import math\n",
        "import json\n",
        "from functools import partial\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "plt.set_cmap('cividis')\n",
        "%matplotlib inline \n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.reset_orig()\n",
        "\n",
        "## tqdm for loading bars\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install pytorch-lightning==1.3.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Setting the seed\n",
        "pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.determinstic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Global seed set to 42\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Device: cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8wpekrychJy"
      },
      "source": [
        "# access to local google drive\n",
        "# source: https://colab.research.google.com/notebooks/io.ipynb#scrollTo=RWSJpsyKqHjH\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOWUXRyhfUjH",
        "outputId": "43a94a88-1e88-499a-b2ef-8d60d67dcec9"
      },
      "source": [
        "%pip freeze"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "absl-py==0.12.0\n",
            "aiohttp==3.7.4.post0\n",
            "alabaster==0.7.12\n",
            "albumentations==0.1.12\n",
            "altair==4.1.0\n",
            "appdirs==1.4.4\n",
            "argon2-cffi==20.1.0\n",
            "arviz==0.11.2\n",
            "astor==0.8.1\n",
            "astropy==4.2.1\n",
            "astunparse==1.6.3\n",
            "async-generator==1.10\n",
            "async-timeout==3.0.1\n",
            "atari-py==0.2.9\n",
            "atomicwrites==1.4.0\n",
            "attrs==21.2.0\n",
            "audioread==2.1.9\n",
            "autograd==1.3\n",
            "Babel==2.9.1\n",
            "backcall==0.2.0\n",
            "beautifulsoup4==4.6.3\n",
            "bleach==3.3.0\n",
            "blis==0.4.1\n",
            "bokeh==2.3.3\n",
            "Bottleneck==1.3.2\n",
            "branca==0.4.2\n",
            "bs4==0.0.1\n",
            "CacheControl==0.12.6\n",
            "cached-property==1.5.2\n",
            "cachetools==4.2.2\n",
            "catalogue==1.0.0\n",
            "certifi==2021.5.30\n",
            "cffi==1.14.6\n",
            "cftime==1.5.0\n",
            "chardet==3.0.4\n",
            "charset-normalizer==2.0.2\n",
            "click==7.1.2\n",
            "cloudpickle==1.3.0\n",
            "cmake==3.12.0\n",
            "cmdstanpy==0.9.5\n",
            "colorcet==2.0.6\n",
            "colorlover==0.3.0\n",
            "community==1.0.0b1\n",
            "contextlib2==0.5.5\n",
            "convertdate==2.3.2\n",
            "coverage==3.7.1\n",
            "coveralls==0.5\n",
            "crcmod==1.7\n",
            "cufflinks==0.17.3\n",
            "cvxopt==1.2.6\n",
            "cvxpy==1.0.31\n",
            "cycler==0.10.0\n",
            "cymem==2.0.5\n",
            "Cython==0.29.23\n",
            "daft==0.0.4\n",
            "dask==2.12.0\n",
            "datascience==0.10.6\n",
            "debugpy==1.0.0\n",
            "decorator==4.4.2\n",
            "defusedxml==0.7.1\n",
            "descartes==1.1.0\n",
            "dill==0.3.4\n",
            "distributed==1.25.3\n",
            "dlib @ file:///dlib-19.18.0-cp37-cp37m-linux_x86_64.whl\n",
            "dm-tree==0.1.6\n",
            "docopt==0.6.2\n",
            "docutils==0.17.1\n",
            "dopamine-rl==1.0.5\n",
            "earthengine-api==0.1.272\n",
            "easydict==1.9\n",
            "ecos==2.0.7.post1\n",
            "editdistance==0.5.3\n",
            "en-core-web-md @ https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz\n",
            "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz\n",
            "entrypoints==0.3\n",
            "ephem==4.0.0.2\n",
            "et-xmlfile==1.1.0\n",
            "fa2==0.3.5\n",
            "fastai==1.0.61\n",
            "fastdtw==0.3.4\n",
            "fastprogress==1.0.0\n",
            "fastrlock==0.6\n",
            "fbprophet==0.7.1\n",
            "feather-format==0.4.1\n",
            "filelock==3.0.12\n",
            "firebase-admin==4.4.0\n",
            "fix-yahoo-finance==0.0.22\n",
            "Flask==1.1.4\n",
            "flatbuffers==1.12\n",
            "folium==0.8.3\n",
            "fsspec==2021.7.0\n",
            "future==0.18.2\n",
            "gast==0.4.0\n",
            "GDAL==2.2.2\n",
            "gdown==3.6.4\n",
            "gensim==3.6.0\n",
            "geographiclib==1.52\n",
            "geopy==1.17.0\n",
            "gin-config==0.4.0\n",
            "glob2==0.7\n",
            "google==2.0.3\n",
            "google-api-core==1.26.3\n",
            "google-api-python-client==1.12.8\n",
            "google-auth==1.32.1\n",
            "google-auth-httplib2==0.0.4\n",
            "google-auth-oauthlib==0.4.4\n",
            "google-cloud-bigquery==1.21.0\n",
            "google-cloud-bigquery-storage==1.1.0\n",
            "google-cloud-core==1.0.3\n",
            "google-cloud-datastore==1.8.0\n",
            "google-cloud-firestore==1.7.0\n",
            "google-cloud-language==1.2.0\n",
            "google-cloud-storage==1.18.1\n",
            "google-cloud-translate==1.5.0\n",
            "google-colab @ file:///colabtools/dist/google-colab-1.0.0.tar.gz\n",
            "google-pasta==0.2.0\n",
            "google-resumable-media==0.4.1\n",
            "googleapis-common-protos==1.53.0\n",
            "googledrivedownloader==0.4\n",
            "graphviz==0.10.1\n",
            "greenlet==1.1.0\n",
            "grpcio==1.34.1\n",
            "gspread==3.0.1\n",
            "gspread-dataframe==3.0.8\n",
            "gym==0.17.3\n",
            "h5py==3.1.0\n",
            "HeapDict==1.0.1\n",
            "hijri-converter==2.1.3\n",
            "holidays==0.10.5.2\n",
            "holoviews==1.14.4\n",
            "html5lib==1.0.1\n",
            "httpimport==0.5.18\n",
            "httplib2==0.17.4\n",
            "httplib2shim==0.0.3\n",
            "humanize==0.5.1\n",
            "hyperopt==0.1.2\n",
            "ideep4py==2.0.0.post3\n",
            "idna==2.10\n",
            "imageio==2.4.1\n",
            "imagesize==1.2.0\n",
            "imbalanced-learn==0.4.3\n",
            "imblearn==0.0\n",
            "imgaug==0.2.9\n",
            "importlib-metadata==4.6.1\n",
            "importlib-resources==5.2.0\n",
            "imutils==0.5.4\n",
            "inflect==2.1.0\n",
            "iniconfig==1.1.1\n",
            "install==1.3.4\n",
            "intel-openmp==2021.3.0\n",
            "intervaltree==2.1.0\n",
            "ipykernel==4.10.1\n",
            "ipython==5.5.0\n",
            "ipython-genutils==0.2.0\n",
            "ipython-sql==0.3.9\n",
            "ipywidgets==7.6.3\n",
            "itsdangerous==1.1.0\n",
            "jax==0.2.17\n",
            "jaxlib @ https://storage.googleapis.com/jax-releases/cuda110/jaxlib-0.1.69+cuda110-cp37-none-manylinux2010_x86_64.whl\n",
            "jdcal==1.4.1\n",
            "jedi==0.18.0\n",
            "jieba==0.42.1\n",
            "Jinja2==2.11.3\n",
            "joblib==1.0.1\n",
            "jpeg4py==0.1.4\n",
            "jsonschema==2.6.0\n",
            "jupyter==1.0.0\n",
            "jupyter-client==5.3.5\n",
            "jupyter-console==5.2.0\n",
            "jupyter-core==4.7.1\n",
            "jupyterlab-pygments==0.1.2\n",
            "jupyterlab-widgets==1.0.0\n",
            "kaggle==1.5.12\n",
            "kapre==0.3.5\n",
            "Keras==2.4.3\n",
            "keras-nightly==2.5.0.dev2021032900\n",
            "Keras-Preprocessing==1.1.2\n",
            "keras-vis==0.4.1\n",
            "kiwisolver==1.3.1\n",
            "korean-lunar-calendar==0.2.1\n",
            "librosa==0.8.1\n",
            "lightgbm==2.2.3\n",
            "llvmlite==0.34.0\n",
            "lmdb==0.99\n",
            "LunarCalendar==0.0.9\n",
            "lxml==4.2.6\n",
            "Markdown==3.3.4\n",
            "MarkupSafe==2.0.1\n",
            "matplotlib==3.2.2\n",
            "matplotlib-inline==0.1.2\n",
            "matplotlib-venn==0.11.6\n",
            "missingno==0.5.0\n",
            "mistune==0.8.4\n",
            "mizani==0.6.0\n",
            "mkl==2019.0\n",
            "mlxtend==0.14.0\n",
            "more-itertools==8.8.0\n",
            "moviepy==0.2.3.5\n",
            "mpmath==1.2.1\n",
            "msgpack==1.0.2\n",
            "multidict==5.1.0\n",
            "multiprocess==0.70.12.2\n",
            "multitasking==0.0.9\n",
            "murmurhash==1.0.5\n",
            "music21==5.5.0\n",
            "natsort==5.5.0\n",
            "nbclient==0.5.3\n",
            "nbconvert==5.6.1\n",
            "nbformat==5.1.3\n",
            "nest-asyncio==1.5.1\n",
            "netCDF4==1.5.7\n",
            "networkx==2.5.1\n",
            "nibabel==3.0.2\n",
            "nltk==3.2.5\n",
            "notebook==5.3.1\n",
            "numba==0.51.2\n",
            "numexpr==2.7.3\n",
            "numpy==1.19.5\n",
            "nvidia-ml-py3==7.352.0\n",
            "oauth2client==4.1.3\n",
            "oauthlib==3.1.1\n",
            "okgrade==0.4.3\n",
            "opencv-contrib-python==4.1.2.30\n",
            "opencv-python==4.1.2.30\n",
            "openpyxl==2.5.9\n",
            "opt-einsum==3.3.0\n",
            "osqp==0.6.2.post0\n",
            "packaging==21.0\n",
            "palettable==3.3.0\n",
            "pandas==1.1.5\n",
            "pandas-datareader==0.9.0\n",
            "pandas-gbq==0.13.3\n",
            "pandas-profiling==1.4.1\n",
            "pandocfilters==1.4.3\n",
            "panel==0.11.3\n",
            "param==1.11.1\n",
            "parso==0.8.2\n",
            "pathlib==1.0.1\n",
            "patsy==0.5.1\n",
            "pexpect==4.8.0\n",
            "pickleshare==0.7.5\n",
            "Pillow==7.1.2\n",
            "pip-tools==4.5.1\n",
            "plac==1.1.3\n",
            "plotly==4.4.1\n",
            "plotnine==0.6.0\n",
            "pluggy==0.7.1\n",
            "pooch==1.4.0\n",
            "portpicker==1.3.9\n",
            "prefetch-generator==1.0.1\n",
            "preshed==3.0.5\n",
            "prettytable==2.1.0\n",
            "progressbar2==3.38.0\n",
            "prometheus-client==0.11.0\n",
            "promise==2.3\n",
            "prompt-toolkit==1.0.18\n",
            "protobuf==3.17.3\n",
            "psutil==5.4.8\n",
            "psycopg2==2.7.6.1\n",
            "ptyprocess==0.7.0\n",
            "py==1.10.0\n",
            "pyarrow==3.0.0\n",
            "pyasn1==0.4.8\n",
            "pyasn1-modules==0.2.8\n",
            "pycocotools==2.0.2\n",
            "pycparser==2.20\n",
            "pyct==0.4.8\n",
            "pydata-google-auth==1.2.0\n",
            "pyDeprecate==0.3.0\n",
            "pydot==1.3.0\n",
            "pydot-ng==2.0.0\n",
            "pydotplus==2.0.2\n",
            "PyDrive==1.3.1\n",
            "pyemd==0.5.1\n",
            "pyerfa==2.0.0\n",
            "pyglet==1.5.0\n",
            "Pygments==2.6.1\n",
            "pygobject==3.26.1\n",
            "pymc3==3.11.2\n",
            "PyMeeus==0.5.11\n",
            "pymongo==3.11.4\n",
            "pymystem3==0.2.0\n",
            "PyOpenGL==3.1.5\n",
            "pyparsing==2.4.7\n",
            "pyrsistent==0.18.0\n",
            "pysndfile==1.3.8\n",
            "PySocks==1.7.1\n",
            "pystan==2.19.1.1\n",
            "pytest==3.6.4\n",
            "python-apt==0.0.0\n",
            "python-chess==0.23.11\n",
            "python-dateutil==2.8.1\n",
            "python-louvain==0.15\n",
            "python-slugify==5.0.2\n",
            "python-utils==2.5.6\n",
            "pytorch-lightning==1.3.4\n",
            "pytz==2018.9\n",
            "pyviz-comms==2.1.0\n",
            "PyWavelets==1.1.1\n",
            "PyYAML==5.4.1\n",
            "pyzmq==22.1.0\n",
            "qdldl==0.1.5.post0\n",
            "qtconsole==5.1.1\n",
            "QtPy==1.9.0\n",
            "regex==2019.12.20\n",
            "requests==2.23.0\n",
            "requests-oauthlib==1.3.0\n",
            "resampy==0.2.2\n",
            "retrying==1.3.3\n",
            "rpy2==3.4.5\n",
            "rsa==4.7.2\n",
            "scikit-image==0.16.2\n",
            "scikit-learn==0.22.2.post1\n",
            "scipy==1.4.1\n",
            "screen-resolution-extra==0.0.0\n",
            "scs==2.1.4\n",
            "seaborn==0.11.1\n",
            "semver==2.13.0\n",
            "Send2Trash==1.7.1\n",
            "setuptools-git==1.2\n",
            "Shapely==1.7.1\n",
            "simplegeneric==0.8.1\n",
            "six==1.15.0\n",
            "sklearn==0.0\n",
            "sklearn-pandas==1.8.0\n",
            "smart-open==5.1.0\n",
            "snowballstemmer==2.1.0\n",
            "sortedcontainers==2.4.0\n",
            "SoundFile==0.10.3.post1\n",
            "spacy==2.2.4\n",
            "Sphinx==1.8.5\n",
            "sphinxcontrib-serializinghtml==1.1.5\n",
            "sphinxcontrib-websupport==1.2.4\n",
            "SQLAlchemy==1.4.20\n",
            "sqlparse==0.4.1\n",
            "srsly==1.0.5\n",
            "statsmodels==0.10.2\n",
            "sympy==1.7.1\n",
            "tables==3.4.4\n",
            "tabulate==0.8.9\n",
            "tblib==1.7.0\n",
            "tensorboard==2.4.1\n",
            "tensorboard-data-server==0.6.1\n",
            "tensorboard-plugin-wit==1.8.0\n",
            "tensorflow @ file:///tensorflow-2.5.0-cp37-cp37m-linux_x86_64.whl\n",
            "tensorflow-datasets==4.0.1\n",
            "tensorflow-estimator==2.5.0\n",
            "tensorflow-gcs-config==2.5.0\n",
            "tensorflow-hub==0.12.0\n",
            "tensorflow-metadata==1.1.0\n",
            "tensorflow-probability==0.13.0\n",
            "termcolor==1.1.0\n",
            "terminado==0.10.1\n",
            "testpath==0.5.0\n",
            "text-unidecode==1.3\n",
            "textblob==0.15.3\n",
            "Theano-PyMC==1.1.2\n",
            "thinc==7.4.0\n",
            "tifffile==2021.7.2\n",
            "toml==0.10.2\n",
            "toolz==0.11.1\n",
            "torch @ https://download.pytorch.org/whl/cu102/torch-1.9.0%2Bcu102-cp37-cp37m-linux_x86_64.whl\n",
            "torchmetrics==0.4.1\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.10.0\n",
            "torchvision @ https://download.pytorch.org/whl/cu102/torchvision-0.10.0%2Bcu102-cp37-cp37m-linux_x86_64.whl\n",
            "tornado==5.1.1\n",
            "tqdm==4.41.1\n",
            "traitlets==5.0.5\n",
            "tweepy==3.10.0\n",
            "typeguard==2.7.1\n",
            "typing-extensions==3.7.4.3\n",
            "tzlocal==1.5.1\n",
            "uritemplate==3.0.1\n",
            "urllib3==1.24.3\n",
            "vega-datasets==0.9.0\n",
            "wasabi==0.8.2\n",
            "wcwidth==0.2.5\n",
            "webencodings==0.5.1\n",
            "Werkzeug==1.0.1\n",
            "widgetsnbextension==3.5.1\n",
            "wordcloud==1.5.0\n",
            "wrapt==1.12.1\n",
            "xarray==0.18.2\n",
            "xgboost==0.90\n",
            "xkit==0.0.0\n",
            "xlrd==1.1.0\n",
            "xlwt==1.3.0\n",
            "yarl==1.6.3\n",
            "yellowbrick==0.9.1\n",
            "zict==2.0.0\n",
            "zipp==3.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40PsB0wezPDZ"
      },
      "source": [
        "## The Transformer architecture\n",
        "\n",
        "In the first part of this notebook, we will implement the Transformer architecture by hand. As the architecture is so popular, there already exists a Pytorch module `nn.Transformer` ([documentation](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)) and a [tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html) on how to use it for next token prediction. However, we will implement it here ourselves, to get through to the smallest details.\n",
        "\n",
        "There are of course many more tutorials out there about attention and Transformers. Below, we list a few that are worth exploring if you are interested in the topic and might want yet another perspective on the topic after this one:\n",
        "\n",
        "* [Transformer: A Novel Neural Network Architecture for Language Understanding (Jakob Uszkoreit, 2017)](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) - The original Google blog post about the Transformer paper, focusing on the application in machine translation.\n",
        "* [The Illustrated Transformer (Jay Alammar, 2018)](http://jalammar.github.io/illustrated-transformer/) - A very popular and great blog post intuitively explaining the Transformer architecture with many nice visualizations. The focus is on NLP.\n",
        "* [Attention? Attention! (Lilian Weng, 2018)](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) - A nice blog post summarizing attention mechanisms in many domains including vision.\n",
        "* [Illustrated: Self-Attention (Raimi Karim, 2019)](https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a) - A nice visualization of the steps of self-attention. Recommended going through if the explanation below is too abstract for you.\n",
        "* [The Transformer family (Lilian Weng, 2020)](https://lilianweng.github.io/lil-log/2020/04/07/the-transformer-family.html) - A very detailed blog post reviewing more variants of Transformers besides the original one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AxCkULhzPDa"
      },
      "source": [
        "### What is Attention?\n",
        "\n",
        "The attention mechanism describes a recent new group of layers in neural networks that has attracted a lot of interest in the past few years, especially in sequence tasks. There are a lot of different possible definitions of \"attention\" in the literature, but the one we will use here is the following: _the attention mechanism describes a weighted average of (sequence) elements with the weights dynamically computed based on an input query and elements' keys_. So what does this exactly mean? The goal is to take an average over the features of multiple elements. However, instead of weighting each element equally, we want to weight them depending on their actual values. In other words, we want to dynamically decide on which inputs we want to \"attend\" more than others. In particular, an attention mechanism has usually four parts we need to specify:\n",
        "\n",
        "* **Query**: The query is a feature vector that describes what we are looking for in the sequence, i.e. what would we maybe want to pay attention to.\n",
        "* **Keys**: For each input element, we have a key which is again a feature vector. This feature vector roughly describes what the element is \"offering\", or when it might be important. The keys should be designed such that we can identify the elements we want to pay attention to based on the query.\n",
        "* **Values**: For each input element, we also have a value vector. This feature vector is the one we want to average over.\n",
        "* **Score function**: To rate which elements we want to pay attention to, we need to specify a score function $f_{attn}$. The score function takes the query and a key as input, and output the score/attention weight of the query-key pair. It is usually implemented by simple similarity metrics like a dot product, or a small MLP.\n",
        "\n",
        "\n",
        "The weights of the average are calculated by a softmax over all score function outputs. Hence, we assign those value vectors a higher weight whose corresponding key is most similar to the query. If we try to describe it with pseudo-math, we can write: \n",
        "\n",
        "$$\n",
        "\\alpha_i = \\frac{\\exp\\left(f_{attn}\\left(\\text{key}_i, \\text{query}\\right)\\right)}{\\sum_j \\exp\\left(f_{attn}\\left(\\text{key}_j, \\text{query}\\right)\\right)}, \\hspace{5mm} \\text{out} = \\sum_i \\alpha_i \\cdot \\text{value}_i\n",
        "$$\n",
        "\n",
        "Visually, we can show the attention over a sequence of words as follows:\n",
        "\n",
        "<center width=\"100%\" style=\"padding:25px\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/attention_example.svg?raw=1\" width=\"750px\"></center>\n",
        "\n",
        "For every word, we have one key and one value vector. The query is compared to all keys with a score function (in this case the dot product) to determine the weights. The softmax is not visualized for simplicity. Finally, the value vectors of all words are averaged using the attention weights.\n",
        "\n",
        "Most attention mechanisms differ in terms of what queries they use, how the key and value vectors are defined, and what score function is used. The attention applied inside the Transformer architecture is called **self-attention**. In self-attention, each sequence element provides a key, value, and query. For each element, we perform an attention layer where based on its query, we check the similarity of the all sequence elements' keys, and returned a different, averaged value vector for each element. We will now go into a bit more detail by first looking at the specific implementation of the attention mechanism which is in the Transformer case the scaled dot product attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0yyxCpUzPDa"
      },
      "source": [
        "### Scaled Dot Product Attention\n",
        "\n",
        "The core concept behind self-attention is the scaled dot product attention. Our goal is to have an attention mechanism with which any element in a sequence can attend to any other while still being efficient to compute. The dot product attention takes as input a set of queries $Q\\in\\mathbb{R}^{T\\times d_k}$, keys $K\\in\\mathbb{R}^{T\\times d_k}$ and values $V\\in\\mathbb{R}^{T\\times d_v}$ where $T$ is the sequence length, and $d_k$ and $d_v$ are the hidden dimensionality for queries/keys and values respectively. For simplicity, we neglect the batch dimension for now. The attention value from element $i$ to $j$ is based on its similarity of the query $Q_i$ and key $K_j$, using the dot product as the similarity metric. In math, we calculate the dot product attention as follows:\n",
        "\n",
        "$$\\text{Attention}(Q,K,V)=\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "The matrix multiplication $QK^T$ performs the dot product for every possible pair of queries and keys, resulting in a matrix of the shape $T\\times T$. Each row represents the attention logits for a specific element $i$ to all other elements in the sequence. On these, we apply a softmax and multiply with the value vector to obtain a weighted mean (the weights being determined by the attention). Another perspective on this attention mechanism offers the computation graph which is visualized below (figure credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/scaled_dot_product_attn.svg?raw=1\" width=\"210px\"></center>\n",
        "\n",
        "One aspect we haven't discussed yet is the scaling factor of $1/\\sqrt{d_k}$. This scaling factor is crucial to maintain an appropriate variance of attention values after initialization. Remember that we intialize our layers with the intention of having equal variance throughout the model, and hence, $Q$ and $K$ might also have a variance close to $1$. However, performing a dot product over two vectors with a variance $\\sigma$ results in a scalar having $d_k$-times higher variance: \n",
        "\n",
        "$$q_i \\sim \\mathcal{N}(0,\\sigma), k_i \\sim \\mathcal{N}(0,\\sigma) \\to \\text{Var}\\left(\\sum_{i=1}^{d_k} q_i\\cdot k_i\\right) = \\sigma\\cdot d_k$$\n",
        "\n",
        "\n",
        "If we do not scale down the variance back to $\\sigma$, the softmax over the logits will already saturate to $1$ for one random element and $0$ for all others. The gradients through the softmax will be close to zero so that we can't learn the parameters appropriately. \n",
        "\n",
        "The block `Mask (opt.)` in the diagram above represents the optional masking of specific entries in the attention matrix. This is for instance used if we stack multiple sequences with different lengths into a batch. To still benefit from parallelization in PyTorch, we pad the sentences to the same length and mask out the padding tokens during the calculation of the attention values. This is usually done by setting the respective attention logits to a very low value. \n",
        "\n",
        "After we have discussed the details of the scaled dot product attention block, we can write a function below which computes the output features given the triple of queries, keys, and values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIrOs7oPNA3O"
      },
      "source": [
        "# Reminder\n",
        "# Initializing : https://pytorch.org/docs/stable/tensors.html\n",
        "print(torch.tensor([[1., -1.], [1., -1.]]))\n",
        "print(torch.tensor(np.array([[1, 2, 3], [4, 5, 6]])))\n",
        "print(\"-\")\n",
        "\n",
        "# https://pytorch.org/docs/stable/generated/torch.transpose.html\n",
        "x = torch.randn(2, 3, 2)\n",
        "print(f\"x={x} \\n dim={x.dim()} \\n size={x.size()} \")\n",
        "print(\"-\")\n",
        "t=torch.transpose(x, 0, 1)\n",
        "print(f\"transpose={t} \\n size={t.size()}\")\n",
        "print(\"-\")\n",
        "t1=torch.transpose(x, -2, -1)\n",
        "print(f\"transpose={t1} \\n size={t1.size()}\")\n",
        "print(\"-\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRh9dR2xP5Mn"
      },
      "source": [
        "# Run This\n",
        "Original Transformer Model Code "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG3ckJqlzPDb"
      },
      "source": [
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    # d_k: hidden dimensionality for q and k\n",
        "    d_k = q.size()[-1]\n",
        "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "    attn_logits = attn_logits / math.sqrt(d_k)\n",
        "    \n",
        "    if mask is not None:\n",
        "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
        "    attention = F.softmax(attn_logits, dim=-1)\n",
        "    \n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmGSOpZ1zPDb"
      },
      "source": [
        "Note that our code above supports any additional dimensionality in front of the sequence length so that we can also use it for batches. However, for a better understanding, let's generate a few random queries, keys, and value vectors, and calculate the attention outputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV6ug11TzPDc"
      },
      "source": [
        "# d_k: hidden dimensionality for q and k\n",
        "# seq_len:\n",
        "seq_len, d_k = 3, 2\n",
        "pl.seed_everything(42)\n",
        "q = torch.randn(seq_len, d_k)\n",
        "k = torch.randn(seq_len, d_k)\n",
        "v = torch.randn(seq_len, d_k)\n",
        "values, attention = scaled_dot_product(q, k, v)\n",
        "print(\"Q\\n\", q)\n",
        "print(\"K\\n\", k)\n",
        "print(\"V\\n\", v)\n",
        "print(\"Values\\n\", values)\n",
        "print(\"Attention\\n\", attention)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByuzDHXCzPDc"
      },
      "source": [
        "Before continuing, make sure you can follow the calculation of the specific values here, and also check it by hand. It is important to fully understand how the scaled dot product attention is calculated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h20BlWk7zPDc"
      },
      "source": [
        "### Multi-Head Attention\n",
        "\n",
        "The scaled dot product attention allows a network to attend over a sequence. However, often there are multiple different aspects a sequence element wants to attend to, and a single weighted average is not a good option for it. This is why we extend the attention mechanisms to multiple heads, i.e. multiple different query-key-value triplets on the same features. Specifically, given a query, key, and value matrix, we transform those into $h$ sub-queries, sub-keys, and sub-values, which we pass through the scaled dot product attention independently. Afterward, we concatenate the heads and combine them with a final weight matrix. Mathematically, we can express this operation as:\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    \\text{Multihead}(Q,K,V) & = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^{O}\\\\\n",
        "    \\text{where } \\text{head}_i & = \\text{Attention}(QW_i^Q,KW_i^K, VW_i^V)\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "We refer to this as Multi-Head Attention layer with the learnable parameters $W_{1...h}^{Q}\\in\\mathbb{R}^{D\\times d_k}$, $W_{1...h}^{K}\\in\\mathbb{R}^{D\\times d_k}$, $W_{1...h}^{V}\\in\\mathbb{R}^{D\\times d_v}$, and $W^{O}\\in\\mathbb{R}^{h\\cdot d_k\\times d_{out}}$ ($D$ being the input dimensionality). Expressed in a computational graph, we can visualize it as below (figure credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/multihead_attention.svg?raw=1\" width=\"230px\"></center>\n",
        "\n",
        "How are we applying a Multi-Head Attention layer in a neural network, where we don't have an arbitrary query, key, and value vector as input? Looking at the computation graph above, a simple but effective implementation is to set the current feature map in a NN, $X\\in\\mathbb{R}^{B\\times T\\times d_{\\text{model}}}$, as $Q$, $K$ and $V$ ($B$ being the batch size, $T$ the sequence length, $d_{\\text{model}}$ the hidden dimensionality of $X$). The consecutive weight matrices $W^{Q}$, $W^{K}$, and $W^{V}$ can transform $X$ to the corresponding feature vectors that represent the queries, keys, and values of the input. Using this approach, we can implement the Multi-Head Attention module below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9TLAxX-WnLx"
      },
      "source": [
        "# Reminder : get linear nn information\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
        "\n",
        "m = nn.Linear(4, 3)\n",
        "l = m.state_dict()\n",
        "print(l)\n",
        "#print(l['weight'])\n",
        "#print(l['bias'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byq-OqQAQocm"
      },
      "source": [
        "# Run This\n",
        "Original Transformer Model Code "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ9cKEjqzPDd"
      },
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
        "        \n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        \n",
        "        # Stack all weight matrices 1...h together for efficiency\n",
        "        # Note that in many implementations you see \"bias=False\" which is optional\n",
        "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
        "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        \n",
        "        self._reset_parameters()\n",
        "        \n",
        "        \n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization, see PyTorch documentation\n",
        "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
        "        self.qkv_proj.bias.data.fill_(0)\n",
        "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
        "        self.o_proj.bias.data.fill_(0)\n",
        "        \n",
        "        \n",
        "    def forward(self, x, mask=None, return_attention=False):\n",
        "        print(f\"Tzur DBG1 x.size()={x.size()}\")\n",
        "        batch_size, seq_length, embed_dim = x.size()\n",
        "        print(f\"Tzur DBG1 batch_size={batch_size} seq_length={seq_length} embed_dim={embed_dim}\")\n",
        "        qkv = self.qkv_proj(x)\n",
        "        \n",
        "        # Separate Q, K, V from linear output\n",
        "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "        \n",
        "        # Determine value outputs\n",
        "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
        "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
        "        values = values.reshape(batch_size, seq_length, embed_dim)\n",
        "        o = self.o_proj(values)\n",
        "        \n",
        "        if return_attention:\n",
        "            return o, attention\n",
        "        else:\n",
        "            return o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDfnBDjTzPDd"
      },
      "source": [
        "One crucial characteristic of the multi-head attention is that it is permutation-equivariant with respect to its inputs. This means that if we switch two input elements in the sequence, e.g. $X_1\\leftrightarrow X_2$ (neglecting the batch dimension for now), the output is exactly the same besides the elements 1 and 2 switched. Hence, the multi-head attention is actually looking at the input not as a sequence, but as a set of elements. This property makes the multi-head attention block and the Transformer architecture so powerful and widely applicable! But what if the order of the input is actually important for solving the task, like language modeling? The answer is to encode the position in the input features, which we will take a closer look at later (topic _Positional encodings_ below).\n",
        "\n",
        "Before moving on to creating the Transformer architecture, we can compare the self-attention operation with our other common layer competitors for sequence data: convolutions and recurrent neural networks. Below you can find a table by [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762) on the complexity per layer, the number of sequential operations, and maximum path length. The complexity is measured by the upper bound of the number of operations to perform, while the maximum path length represents the maximum number of steps a forward or backward signal has to traverse to reach any other position. The lower this length, the better gradient signals can backpropagate for long-range dependencies. Let's take a look at the table below:\n",
        "\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/comparison_conv_rnn.svg?raw=1\" width=\"600px\"></center>\n",
        "\n",
        "$n$ is the sequence length, $d$ is the representation dimension and $k$ is the kernel size of convolutions. In contrast to recurrent networks, the self-attention layer can parallelize all its operations making it much faster to execute for smaller sequence lengths. However, when the sequence length exceeds the hidden dimensionality, self-attention becomes more expensive than RNNs. One way of reducing the computational cost for long sequences is by restricting the self-attention to a neighborhood of inputs to attend over, denoted by $r$. Nevertheless, there has been recently a lot of work on more efficient Transformer architectures that still allow long dependencies, of which you can find an overview in the paper by [Tay et al. (2020)](https://arxiv.org/abs/2009.06732) if interested."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jva2HekLzPDd"
      },
      "source": [
        "### Transformer Encoder\n",
        "\n",
        "Next, we will look at how to apply the multi-head attention blog inside the Transformer architecture. Originally, the Transformer model was designed for machine translation. Hence, it got an encoder-decoder structure where the encoder takes as input the sentence in the original language and generates an attention-based representation. On the other hand, the decoder attends over the encoded information and generates the translated sentence in an autoregressive manner, as in a standard RNN. While this structure is extremely useful for Sequence-to-Sequence tasks with the necessity of autoregressive decoding, we will focus here on the encoder part. Many advances in NLP have been made using pure encoder-based Transformer models (if interested, models include the [BERT](https://arxiv.org/abs/1810.04805)-family, the [Vision Transformer](https://arxiv.org/abs/2010.11929), and more), and in our tutorial, we will also mainly focus on the encoder part. If you have understood the encoder architecture, the decoder is a very small step to implement as well. The full Transformer architecture looks as follows (figure credit - [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)).:\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/transformer_architecture.svg?raw=1\" width=\"400px\"></center>\n",
        "\n",
        "The encoder consists of $N$ identical blocks that are applied in sequence. Taking as input $x$, it is first passed through a Multi-Head Attention block as we have implemented above. The output is added to the original input using a residual connection, and we apply a consecutive Layer Normalization on the sum. Overall, it calculates $\\text{LayerNorm}(x+\\text{Multihead}(x,x,x))$ ($x$ being $Q$, $K$ and $V$ input to the attention layer). The residual connection is crucial in the Transformer architecture for two reasons: \n",
        "\n",
        "1. Similar to ResNets, Transformers are designed to be very deep. Some models contain more than 24 blocks in the encoder. Hence, the residual connections are crucial for enabling a smooth gradient flow through the model.\n",
        "2. Without the residual connection, the information about the original sequence is lost. Remember that the Multi-Head Attention layer ignores the position of elements in a sequence, and can only learn it based on the input features. Removing the residual connections would mean that this information is lost after the first attention layer (after initialization), and with a randomly initialized query and key vector, the output vectors for position $i$ has no relation to its original input. All outputs of the attention are likely to represent similar/same information, and there is no chance for the model to distinguish which information came from which input element. An alternative option to residual connection would be to fix at least one head to focus on its original input, but this is very inefficient and does not have the benefit of the improved gradient flow.\n",
        "\n",
        "The Layer Normalization also plays an important role in the Transformer architecture as it enables faster training and provides small regularization. Additionally, it ensures that the features are in a similar magnitude among the elements in the sequence. We are not using Batch Normalization because it depends on the batch size which is often small with Transformers (they require a lot of GPU memory), and BatchNorm has shown to perform particularly bad in language as the features of words tend to have a much higher variance (there are many, very rare words which need to be considered for a good distribution estimate).\n",
        "\n",
        "Additionally to the Multi-Head Attention, a small fully connected feed-forward network is added to the model, which is applied to each position separately and identically. Specifically, the model uses a Linear$\\to$ReLU$\\to$Linear MLP. The full transformation including the residual connection can be expressed as:  \n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    \\text{FFN}(x) & = \\max(0, xW_1+b_1)W_2 + b_2\\\\\n",
        "    x & = \\text{LayerNorm}(x + \\text{FFN}(x))\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "This MLP adds extra complexity to the model and allows transformations on each sequence element separately. You can imagine as this allows the model to \"post-process\" the new information added by the previous Multi-Head Attention, and prepare it for the next attention block. Usually, the inner dimensionality of the MLP is 2-8$\\times$ larger than $d_{\\text{model}}$, i.e. the dimensionality of the original input $x$. The general advantage of a wider layer instead of a narrow, multi-layer MLP is the faster, parallelizable execution.\n",
        "\n",
        "Finally, after looking at all parts of the encoder architecture, we can start implementing it below. We first start by implementing a single encoder block. Additionally to the layers described above, we will add dropout layers in the MLP and on the output of the MLP and Multi-Head Attention for regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETCyIJ53Qv8I"
      },
      "source": [
        "# Run This \n",
        "Original Transformer Model Code \n",
        "\n",
        "**reminder**:\n",
        "in this implementation input_dim = embed_dim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g_tq58IzPDe"
      },
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0, embed_dim=0):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            input_dim - Dimensionality of the input\n",
        "            num_heads - Number of heads to use in the attention block\n",
        "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
        "            dropout - Dropout probability to use in the dropout layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # Attention layer\n",
        "        # in this implementation input_dim = embed_dim\n",
        "        if embed_dim == 0:\n",
        "            embed_dim = input_dim\n",
        "\n",
        "        self.self_attn = MultiheadAttention(input_dim, embed_dim, num_heads)\n",
        "        \n",
        "        # Two-layer MLP\n",
        "        self.linear_net = nn.Sequential(\n",
        "            nn.Linear(input_dim, dim_feedforward),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim_feedforward, input_dim)\n",
        "        )\n",
        "        \n",
        "        # Layers to apply in between the main layers\n",
        "        self.norm1 = nn.LayerNorm(input_dim)\n",
        "        self.norm2 = nn.LayerNorm(input_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        # Attention part\n",
        "        attn_out = self.self_attn(x, mask=mask)\n",
        "        x = x + self.dropout(attn_out)\n",
        "        x = self.norm1(x)\n",
        "        \n",
        "        # MLP part\n",
        "        linear_out = self.linear_net(x)\n",
        "        x = x + self.dropout(linear_out)\n",
        "        x = self.norm2(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSgKyRpkzPDe"
      },
      "source": [
        "Based on this block, we can implement a module for the full Transformer encoder. Additionally to a forward function that iterates through the sequence of encoder blocks, we also provide a function called `get_attention_maps`. The idea of this function is to return the attention probabilities for all Multi-Head Attention blocks in the encoder. This helps us in understanding, and in a sense, explaining the model. However, the attention probabilities should be interpreted with a grain of salt as it does not necessarily reflect the true interpretation of the model (there is a series of papers about this, including [Attention is not Explanation](https://arxiv.org/abs/1902.10186) and [Attention is not not Explanation](https://arxiv.org/abs/1908.04626))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiysOr1YQ0yY"
      },
      "source": [
        "# Run This\n",
        "Original Transformer Model Code "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH_F9TqFzPDe"
      },
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_layers, **block_args):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
        "        \n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        for l in self.layers:\n",
        "            x = l(x, mask=mask)\n",
        "        return x\n",
        "    \n",
        "    \n",
        "    def get_attention_maps(self, x, mask=None):\n",
        "        attention_maps = []\n",
        "        for l in self.layers:\n",
        "            _, attn_map = l.self_attn(x, mask=mask, return_attention=True)\n",
        "            attention_maps.append(attn_map)\n",
        "            x = l(x)\n",
        "        return attention_maps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9Mv5avjzPDe"
      },
      "source": [
        "### Positional encoding\n",
        "\n",
        "We have discussed before that the Multi-Head Attention block is permutation-equivariant, and cannot distinguish whether an input comes before another one in the sequence or not. In tasks like language understanding, however, the position is important for interpreting the input words. The position information can therefore be added via the input features. We could learn a embedding for every possible position, but this would not generalize to a dynamical input sequence length. Hence, the better option is to use feature patterns that the network can identify from the features and potentially generalize to larger sequences. The specific pattern chosen by Vaswani et al. are sine and cosine functions of different frequencies, as follows:\n",
        "\n",
        "$$\n",
        "PE_{(pos,i)} = \\begin{cases}\n",
        "    \\sin\\left(\\frac{pos}{10000^{i/d_{\\text{model}}}}\\right) & \\text{if}\\hspace{3mm} i \\text{ mod } 2=0\\\\\n",
        "    \\cos\\left(\\frac{pos}{10000^{(i-1)/d_{\\text{model}}}}\\right) & \\text{otherwise}\\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "$PE_{(pos,i)}$ represents the position encoding at position $pos$ in the sequence, and hidden dimensionality $i$. These values, concatenated for all hidden dimensions, are added to the original input features (in the Transformer visualization above, see \"Positional encoding\"), and constitute the position information. We distinguish between even ($i \\text{ mod } 2=0$) and uneven ($i \\text{ mod } 2=1$) hidden dimensionalities where we apply a sine/cosine respectively. The intuition behind this encoding is that you can represent $PE_{(pos+k,:)}$ as a linear function of $PE_{(pos,:)}$, which might allow the model to easily attend to relative positions. The wavelengths in different dimensions range from $2\\pi$ to $10000\\cdot 2\\pi$.\n",
        "\n",
        "The positional encoding is implemented below. The code is taken from the [PyTorch tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html#define-the-model) about Transformers on NLP and adjusted for our purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0e9QzcWQ7GP"
      },
      "source": [
        "# Run This \n",
        "Original Transformer Model Code "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOc2g0y4zPDf"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        \"\"\"\n",
        "        Inputs\n",
        "            d_model - Hidden dimensionality of the input.\n",
        "            max_len - Maximum length of a sequence to expect.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        \n",
        "        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n",
        "        # Used for tensors that need to be on the same device as the module.\n",
        "        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model) \n",
        "        self.register_buffer('pe', pe, persistent=False)\n",
        "\n",
        "        self.flag = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Tzur debug\n",
        "        if self.flag:\n",
        "            print(x.shape)\n",
        "            self.flag = False\n",
        "            \n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REgq5Zn6zPDf"
      },
      "source": [
        "To understand the positional encoding, we can visualize it below. We will generate an image of the positional encoding over hidden dimensionality and position in a sequence. Each pixel, therefore, represents the change of the input feature we perform to encode the specific position. Let's do it below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SswSHYuZzPDf"
      },
      "source": [
        "encod_block = PositionalEncoding(d_model=48, max_len=96)\n",
        "pe = encod_block.pe.squeeze().T.cpu().numpy()\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,3))\n",
        "pos = ax.imshow(pe, cmap=\"RdGy\", extent=(1,pe.shape[1]+1,pe.shape[0]+1,1))\n",
        "fig.colorbar(pos, ax=ax)\n",
        "ax.set_xlabel(\"Position in sequence\")\n",
        "ax.set_ylabel(\"Hidden dimension\")\n",
        "ax.set_title(\"Positional encoding over hidden dimensions\")\n",
        "ax.set_xticks([1]+[i*10 for i in range(1,1+pe.shape[1]//10)])\n",
        "ax.set_yticks([1]+[i*10 for i in range(1,1+pe.shape[0]//10)])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOCxHydVzPDg"
      },
      "source": [
        "You can clearly see the sine and cosine waves with different wavelengths that encode the position in the hidden dimensions. Specifically, we can look at the sine/cosine wave for each hidden dimension separately, to get a better intuition of the pattern. Below we visualize the positional encoding for the hidden dimensions $1$, $2$, $3$ and $4$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9aqYluRzPDg"
      },
      "source": [
        "sns.set_theme()\n",
        "fig, ax = plt.subplots(2, 2, figsize=(12,4))\n",
        "ax = [a for a_list in ax for a in a_list]\n",
        "for i in range(len(ax)):\n",
        "    ax[i].plot(np.arange(1,17), pe[i,:16], color='C%i'%i, marker=\"o\", markersize=6, markeredgecolor=\"black\")\n",
        "    ax[i].set_title(\"Encoding in hidden dimension %i\" % (i+1))\n",
        "    ax[i].set_xlabel(\"Position in sequence\", fontsize=10)\n",
        "    ax[i].set_ylabel(\"Positional encoding\", fontsize=10)\n",
        "    ax[i].set_xticks(np.arange(1,17))\n",
        "    ax[i].tick_params(axis='both', which='major', labelsize=10)\n",
        "    ax[i].tick_params(axis='both', which='minor', labelsize=8)\n",
        "    ax[i].set_ylim(-1.2, 1.2)\n",
        "fig.subplots_adjust(hspace=0.8)\n",
        "sns.reset_orig()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn4HebVqzPDg"
      },
      "source": [
        "As we can see, the patterns between the hidden dimension $1$ and $2$ only differ in the starting angle. The wavelength is $2\\pi$, hence the repetition after position $6$. The hidden dimensions $2$ and $3$ have about twice the wavelength. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Anh5s5PfzPDg"
      },
      "source": [
        "### Learning rate warm-up\n",
        "\n",
        "One commonly used technique for training a Transformer is learning rate warm-up. This means that we gradually increase the learning rate from 0 on to our originally specified learning rate in the first few iterations. Thus, we slowly start learning instead of taking very large steps from the beginning. In fact, training a deep Transformer without learning rate warm-up can make the model diverge and achieve a much worse performance on training and testing. Take for instance the following plot by [Liu et al. (2019)](https://arxiv.org/pdf/1908.03265.pdf) comparing Adam-vanilla (i.e. Adam without warm-up) vs Adam with a warm-up:\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/warmup_loss_plot.svg?raw=1\" width=\"350px\"></center>\n",
        "\n",
        "Clearly, the warm-up is a crucial hyperparameter in the Transformer architecture. Why is it so important? There are currently two common explanations. Firstly, Adam uses the bias correction factors which however can lead to a higher variance in the adaptive learning rate during the first iterations. Improved optimizers like [RAdam](https://arxiv.org/abs/1908.03265) have been shown to overcome this issue, not requiring warm-up for training Transformers. Secondly, the iteratively applied Layer Normalization across layers can lead to very high gradients during the first iterations, which can be solved by using [Pre-Layer Normalization](https://proceedings.icml.cc/static/paper_files/icml/2020/328-Paper.pdf) (similar to Pre-Activation ResNet), or replacing Layer Normalization by other techniques ([Adaptive Normalization](https://proceedings.icml.cc/static/paper_files/icml/2020/328-Paper.pdf), [Power Normalization](https://arxiv.org/abs/2003.07845)). \n",
        "\n",
        "Nevertheless, many applications and papers still use the original Transformer architecture with Adam, because warm-up is a simple, yet effective way of solving the gradient problem in the first iterations. There are many different schedulers we could use. For instance, the original Transformer paper used an exponential decay scheduler with a warm-up. However, the currently most popular scheduler is the cosine warm-up scheduler, which combines warm-up with a cosine-shaped learning rate decay. We can implement it below, and visualize the learning rate factor over epochs. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwRALBKfRJEI"
      },
      "source": [
        "# Run This\n",
        "Original Transformer Model Code "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojcb4NRBzPDh"
      },
      "source": [
        "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
        "    \n",
        "    def __init__(self, optimizer, warmup, max_iters):\n",
        "        self.warmup = warmup\n",
        "        self.max_num_iters = max_iters\n",
        "        super().__init__(optimizer)\n",
        "        \n",
        "    def get_lr(self):\n",
        "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
        "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
        "    \n",
        "    def get_lr_factor(self, epoch):\n",
        "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
        "        if epoch <= self.warmup:\n",
        "            lr_factor *= epoch * 1.0 / self.warmup\n",
        "        return lr_factor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "Avgh9RLgzPDh",
        "outputId": "edc73fbf-338c-42f8-b89c-07350c446baf"
      },
      "source": [
        "# Needed for initializing the lr scheduler\n",
        "p = nn.Parameter(torch.empty(4,4))\n",
        "optimizer = optim.Adam([p], lr=1e-3)\n",
        "lr_scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=100, max_iters=2000)\n",
        "\n",
        "# Plotting\n",
        "epochs = list(range(2000))\n",
        "sns.set()\n",
        "plt.figure(figsize=(8,3))\n",
        "plt.plot(epochs, [lr_scheduler.get_lr_factor(e) for e in epochs])\n",
        "plt.ylabel(\"Learning rate factor\")\n",
        "plt.xlabel(\"Iterations (in batches)\")\n",
        "plt.title(\"Cosine Warm-up Learning Rate Scheduler\")\n",
        "plt.show()\n",
        "sns.reset_orig()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/pdf": "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\n",
            "text/plain": [
              "<Figure size 576x216 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"228.357813pt\" version=\"1.1\" viewBox=\"0 0 503.407187 228.357813\" width=\"503.407187pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 228.357813 \nL 503.407187 228.357813 \nL 503.407187 0 \nL 0 0 \nz\n\" style=\"fill:#ffffff;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 49.807188 185.398125 \nL 496.207187 185.398125 \nL 496.207187 22.318125 \nL 49.807188 22.318125 \nz\n\" style=\"fill:#eaeaf2;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#p1bc1d8aa86)\" d=\"M 70.098097 185.398125 \nL 70.098097 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(66.598722 203.256406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <path clip-path=\"url(#p1bc1d8aa86)\" d=\"M 120.850746 185.398125 \nL 120.850746 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_2\">\n      <!-- 250 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(110.352621 203.256406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#p1bc1d8aa86)\" d=\"M 171.603395 185.398125 \nL 171.603395 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_3\">\n      <!-- 500 -->\n      <g style=\"fill:#262626;\" transform=\"translate(161.10527 203.256406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <path clip-path=\"url(#p1bc1d8aa86)\" d=\"M 222.356044 185.398125 \nL 222.356044 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_4\">\n      <!-- 750 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(211.857919 203.256406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#p1bc1d8aa86)\" d=\"M 273.108693 185.398125 \nL 273.108693 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_5\">\n      <!-- 1000 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(259.111193 203.256406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <path clip-path=\"url(#p1bc1d8aa86)\" d=\"M 323.861342 185.398125 \nL 323.861342 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1250 -->\n      <g style=\"fill:#262626;\" transform=\"translate(309.863842 203.256406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#p1bc1d8aa86)\" d=\"M 374.613991 185.398125 \nL 374.613991 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_7\">\n      <!-- 1500 -->\n      <g style=\"fill:#262626;\" transform=\"translate(360.616491 203.256406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <path clip-path=\"url(#p1bc1d8aa86)\" d=\"M 425.36664 185.398125 \nL 425.36664 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_8\">\n      <!-- 1750 -->\n      <g style=\"fill:#262626;\" transform=\"translate(411.36914 203.256406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#p1bc1d8aa86)\" d=\"M 476.119289 185.398125 \nL 476.119289 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_9\">\n      <!-- 2000 -->\n      <g style=\"fill:#262626;\" transform=\"translate(462.121789 203.256406)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_10\">\n     <!-- Iterations (in batches) -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-73\"/>\n      <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n      <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n      <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n      <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n      <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n      <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 31 75.875 \nQ 24.46875 64.65625 21.28125 53.65625 \nQ 18.109375 42.671875 18.109375 31.390625 \nQ 18.109375 20.125 21.3125 9.0625 \nQ 24.515625 -2 31 -13.1875 \nL 23.1875 -13.1875 \nQ 15.875 -1.703125 12.234375 9.375 \nQ 8.59375 20.453125 8.59375 31.390625 \nQ 8.59375 42.28125 12.203125 53.3125 \nQ 15.828125 64.359375 23.1875 75.875 \nz\n\" id=\"DejaVuSans-40\"/>\n      <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n      <path d=\"M 48.78125 52.59375 \nL 48.78125 44.1875 \nQ 44.96875 46.296875 41.140625 47.34375 \nQ 37.3125 48.390625 33.40625 48.390625 \nQ 24.65625 48.390625 19.8125 42.84375 \nQ 14.984375 37.3125 14.984375 27.296875 \nQ 14.984375 17.28125 19.8125 11.734375 \nQ 24.65625 6.203125 33.40625 6.203125 \nQ 37.3125 6.203125 41.140625 7.25 \nQ 44.96875 8.296875 48.78125 10.40625 \nL 48.78125 2.09375 \nQ 45.015625 0.34375 40.984375 -0.53125 \nQ 36.96875 -1.421875 32.421875 -1.421875 \nQ 20.0625 -1.421875 12.78125 6.34375 \nQ 5.515625 14.109375 5.515625 27.296875 \nQ 5.515625 40.671875 12.859375 48.328125 \nQ 20.21875 56 33.015625 56 \nQ 37.15625 56 41.109375 55.140625 \nQ 45.0625 54.296875 48.78125 52.59375 \nz\n\" id=\"DejaVuSans-99\"/>\n      <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-104\"/>\n      <path d=\"M 8.015625 75.875 \nL 15.828125 75.875 \nQ 23.140625 64.359375 26.78125 53.3125 \nQ 30.421875 42.28125 30.421875 31.390625 \nQ 30.421875 20.453125 26.78125 9.375 \nQ 23.140625 -1.703125 15.828125 -13.1875 \nL 8.015625 -13.1875 \nQ 14.5 -2 17.703125 9.0625 \nQ 20.90625 20.125 20.90625 31.390625 \nQ 20.90625 42.671875 17.703125 53.65625 \nQ 14.5 64.65625 8.015625 75.875 \nz\n\" id=\"DejaVuSans-41\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(206.709062 218.662188)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"29.492188\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"68.701172\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"130.224609\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"171.337891\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"232.617188\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"271.826172\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"299.609375\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"360.791016\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"424.169922\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"476.269531\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"508.056641\" xlink:href=\"#DejaVuSans-40\"/>\n      <use x=\"547.070312\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"574.853516\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"638.232422\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"670.019531\" xlink:href=\"#DejaVuSans-98\"/>\n      <use x=\"733.496094\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"794.775391\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"833.984375\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"888.964844\" xlink:href=\"#DejaVuSans-104\"/>\n      <use x=\"952.34375\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"1013.867188\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"1065.966797\" xlink:href=\"#DejaVuSans-41\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_10\">\n      <path clip-path=\"url(#p1bc1d8aa86)\" d=\"M 49.807188 177.985398 \nL 496.207187 177.985398 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.0 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 182.164538)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#p1bc1d8aa86)\" d=\"M 49.807188 148.150832 \nL 496.207187 148.150832 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.2 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 152.329973)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_12\">\n      <path clip-path=\"url(#p1bc1d8aa86)\" d=\"M 49.807188 118.316267 \nL 496.207187 118.316267 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.4 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 122.495407)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#p1bc1d8aa86)\" d=\"M 49.807188 88.481701 \nL 496.207187 88.481701 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.6 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 92.660842)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_14\">\n      <path clip-path=\"url(#p1bc1d8aa86)\" d=\"M 49.807188 58.647135 \nL 496.207187 58.647135 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_15\">\n      <!-- 0.8 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 62.826276)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_15\">\n      <path clip-path=\"url(#p1bc1d8aa86)\" d=\"M 49.807188 28.81257 \nL 496.207187 28.81257 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:round;\"/>\n     </g>\n     <g id=\"text_16\">\n      <!-- 1.0 -->\n      <g style=\"fill:#262626;\" transform=\"translate(22.81375 32.99171)scale(0.11 -0.11)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- Learning rate factor -->\n     <defs>\n      <path d=\"M 9.8125 72.90625 \nL 19.671875 72.90625 \nL 19.671875 8.296875 \nL 55.171875 8.296875 \nL 55.171875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-76\"/>\n      <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n      <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n     </defs>\n     <g style=\"fill:#262626;\" transform=\"translate(16.318125 163.486875)rotate(-90)scale(0.12 -0.12)\">\n      <use xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"53.962891\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"115.486328\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"176.765625\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"216.128906\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"279.507812\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"307.291016\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"370.669922\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"434.146484\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"465.933594\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"507.046875\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"568.326172\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"607.535156\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"669.058594\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"700.845703\" xlink:href=\"#DejaVuSans-102\"/>\n      <use x=\"736.050781\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"797.330078\" xlink:href=\"#DejaVuSans-99\"/>\n      <use x=\"852.310547\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"891.519531\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"952.701172\" xlink:href=\"#DejaVuSans-114\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p1bc1d8aa86)\" d=\"M 70.098097 177.985398 \nL 89.587114 35.592052 \nL 90.399156 29.730852 \nL 97.504527 30.48331 \nL 104.609898 31.456105 \nL 111.918279 32.683463 \nL 119.226661 34.136891 \nL 126.738053 35.861378 \nL 134.249445 37.813933 \nL 141.963848 40.049729 \nL 149.881261 42.57891 \nL 158.001685 45.409884 \nL 166.325119 48.54904 \nL 174.851564 52.000456 \nL 183.78403 55.855814 \nL 193.122518 60.128835 \nL 203.070037 64.928363 \nL 213.626588 70.270225 \nL 225.198192 76.378163 \nL 238.19087 83.491721 \nL 254.025697 92.425912 \nL 281.432127 108.199226 \nL 303.154261 120.582905 \nL 317.771024 128.66423 \nL 330.15467 135.262305 \nL 341.320253 140.962432 \nL 351.673794 145.998525 \nL 361.418302 150.489325 \nL 370.553779 154.456898 \nL 379.283235 158.01014 \nL 387.80968 161.24043 \nL 395.930104 164.08312 \nL 403.847517 166.624241 \nL 411.561919 168.872118 \nL 419.276322 170.886789 \nL 426.787714 172.61752 \nL 434.096096 174.077165 \nL 441.404477 175.310859 \nL 448.712859 176.314657 \nL 455.818229 177.067115 \nL 462.9236 177.596962 \nL 470.028971 177.902597 \nL 475.916278 177.985306 \nL 475.916278 177.985306 \n\" style=\"fill:none;stroke:#4c72b0;stroke-linecap:round;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 49.807188 185.398125 \nL 49.807188 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 496.207187 185.398125 \nL 496.207187 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 49.807187 185.398125 \nL 496.207188 185.398125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 49.807187 22.318125 \nL 496.207188 22.318125 \n\" style=\"fill:none;stroke:#ffffff;stroke-linecap:square;stroke-linejoin:miter;stroke-width:1.25;\"/>\n   </g>\n   <g id=\"text_18\">\n    <!-- Cosine Warm-up Learning Rate Scheduler -->\n    <defs>\n     <path d=\"M 64.40625 67.28125 \nL 64.40625 56.890625 \nQ 59.421875 61.53125 53.78125 63.8125 \nQ 48.140625 66.109375 41.796875 66.109375 \nQ 29.296875 66.109375 22.65625 58.46875 \nQ 16.015625 50.828125 16.015625 36.375 \nQ 16.015625 21.96875 22.65625 14.328125 \nQ 29.296875 6.6875 41.796875 6.6875 \nQ 48.140625 6.6875 53.78125 8.984375 \nQ 59.421875 11.28125 64.40625 15.921875 \nL 64.40625 5.609375 \nQ 59.234375 2.09375 53.4375 0.328125 \nQ 47.65625 -1.421875 41.21875 -1.421875 \nQ 24.65625 -1.421875 15.125 8.703125 \nQ 5.609375 18.84375 5.609375 36.375 \nQ 5.609375 53.953125 15.125 64.078125 \nQ 24.65625 74.21875 41.21875 74.21875 \nQ 47.75 74.21875 53.53125 72.484375 \nQ 59.328125 70.75 64.40625 67.28125 \nz\n\" id=\"DejaVuSans-67\"/>\n     <path d=\"M 3.328125 72.90625 \nL 13.28125 72.90625 \nL 28.609375 11.28125 \nL 43.890625 72.90625 \nL 54.984375 72.90625 \nL 70.3125 11.28125 \nL 85.59375 72.90625 \nL 95.609375 72.90625 \nL 77.296875 0 \nL 64.890625 0 \nL 49.515625 63.28125 \nL 33.984375 0 \nL 21.578125 0 \nz\n\" id=\"DejaVuSans-87\"/>\n     <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n     <path d=\"M 4.890625 31.390625 \nL 31.203125 31.390625 \nL 31.203125 23.390625 \nL 4.890625 23.390625 \nz\n\" id=\"DejaVuSans-45\"/>\n     <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n     <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n     <path d=\"M 44.390625 34.1875 \nQ 47.5625 33.109375 50.5625 29.59375 \nQ 53.5625 26.078125 56.59375 19.921875 \nL 66.609375 0 \nL 56 0 \nL 46.6875 18.703125 \nQ 43.0625 26.03125 39.671875 28.421875 \nQ 36.28125 30.8125 30.421875 30.8125 \nL 19.671875 30.8125 \nL 19.671875 0 \nL 9.8125 0 \nL 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.578125 72.90625 50.734375 67.671875 \nQ 56.890625 62.453125 56.890625 51.90625 \nQ 56.890625 45.015625 53.6875 40.46875 \nQ 50.484375 35.9375 44.390625 34.1875 \nz\nM 19.671875 64.796875 \nL 19.671875 38.921875 \nL 32.078125 38.921875 \nQ 39.203125 38.921875 42.84375 42.21875 \nQ 46.484375 45.515625 46.484375 51.90625 \nQ 46.484375 58.296875 42.84375 61.546875 \nQ 39.203125 64.796875 32.078125 64.796875 \nz\n\" id=\"DejaVuSans-82\"/>\n     <path d=\"M 53.515625 70.515625 \nL 53.515625 60.890625 \nQ 47.90625 63.578125 42.921875 64.890625 \nQ 37.9375 66.21875 33.296875 66.21875 \nQ 25.25 66.21875 20.875 63.09375 \nQ 16.5 59.96875 16.5 54.203125 \nQ 16.5 49.359375 19.40625 46.890625 \nQ 22.3125 44.4375 30.421875 42.921875 \nL 36.375 41.703125 \nQ 47.40625 39.59375 52.65625 34.296875 \nQ 57.90625 29 57.90625 20.125 \nQ 57.90625 9.515625 50.796875 4.046875 \nQ 43.703125 -1.421875 29.984375 -1.421875 \nQ 24.8125 -1.421875 18.96875 -0.25 \nQ 13.140625 0.921875 6.890625 3.21875 \nL 6.890625 13.375 \nQ 12.890625 10.015625 18.65625 8.296875 \nQ 24.421875 6.59375 29.984375 6.59375 \nQ 38.421875 6.59375 43.015625 9.90625 \nQ 47.609375 13.234375 47.609375 19.390625 \nQ 47.609375 24.75 44.3125 27.78125 \nQ 41.015625 30.8125 33.5 32.328125 \nL 27.484375 33.5 \nQ 16.453125 35.6875 11.515625 40.375 \nQ 6.59375 45.0625 6.59375 53.421875 \nQ 6.59375 63.09375 13.40625 68.65625 \nQ 20.21875 74.21875 32.171875 74.21875 \nQ 37.3125 74.21875 42.625 73.28125 \nQ 47.953125 72.359375 53.515625 70.515625 \nz\n\" id=\"DejaVuSans-83\"/>\n     <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n     <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n    </defs>\n    <g style=\"fill:#262626;\" transform=\"translate(148.179062 16.318125)scale(0.12 -0.12)\">\n     <use xlink:href=\"#DejaVuSans-67\"/>\n     <use x=\"69.824219\" xlink:href=\"#DejaVuSans-111\"/>\n     <use x=\"131.005859\" xlink:href=\"#DejaVuSans-115\"/>\n     <use x=\"183.105469\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"210.888672\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"274.267578\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"335.791016\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"367.578125\" xlink:href=\"#DejaVuSans-87\"/>\n     <use x=\"460.080078\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"521.359375\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"560.722656\" xlink:href=\"#DejaVuSans-109\"/>\n     <use x=\"658.134766\" xlink:href=\"#DejaVuSans-45\"/>\n     <use x=\"694.21875\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"757.597656\" xlink:href=\"#DejaVuSans-112\"/>\n     <use x=\"821.074219\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"852.861328\" xlink:href=\"#DejaVuSans-76\"/>\n     <use x=\"906.824219\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"968.347656\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1029.626953\" xlink:href=\"#DejaVuSans-114\"/>\n     <use x=\"1068.990234\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"1132.369141\" xlink:href=\"#DejaVuSans-105\"/>\n     <use x=\"1160.152344\" xlink:href=\"#DejaVuSans-110\"/>\n     <use x=\"1223.53125\" xlink:href=\"#DejaVuSans-103\"/>\n     <use x=\"1287.007812\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1318.794922\" xlink:href=\"#DejaVuSans-82\"/>\n     <use x=\"1386.027344\" xlink:href=\"#DejaVuSans-97\"/>\n     <use x=\"1447.306641\" xlink:href=\"#DejaVuSans-116\"/>\n     <use x=\"1486.515625\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1548.039062\" xlink:href=\"#DejaVuSans-32\"/>\n     <use x=\"1579.826172\" xlink:href=\"#DejaVuSans-83\"/>\n     <use x=\"1643.302734\" xlink:href=\"#DejaVuSans-99\"/>\n     <use x=\"1698.283203\" xlink:href=\"#DejaVuSans-104\"/>\n     <use x=\"1761.662109\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"1823.185547\" xlink:href=\"#DejaVuSans-100\"/>\n     <use x=\"1886.662109\" xlink:href=\"#DejaVuSans-117\"/>\n     <use x=\"1950.041016\" xlink:href=\"#DejaVuSans-108\"/>\n     <use x=\"1977.824219\" xlink:href=\"#DejaVuSans-101\"/>\n     <use x=\"2039.347656\" xlink:href=\"#DejaVuSans-114\"/>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p1bc1d8aa86\">\n   <rect height=\"163.08\" width=\"446.4\" x=\"49.807188\" y=\"22.318125\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tP9l_SBjzPDh"
      },
      "source": [
        "In the first 100 iterations, we increase the learning rate factor from 0 to 1, whereas for all later iterations, we decay it using the cosine wave. Pre-implementations of this scheduler can be found in the popular NLP Transformer library [huggingface](https://huggingface.co/transformers/main_classes/optimizer_schedules.html?highlight=cosine#transformers.get_cosine_schedule_with_warmup)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfaSxGO_zPDh"
      },
      "source": [
        "### PyTorch Lightning Module\n",
        "\n",
        "Finally, we can embed the Transformer architecture into a PyTorch lightning module. From Tutorial 5, you know that PyTorch Lightning simplifies our training and test code, as well as structures the code nicely in separate functions. We will implement a template for a classifier based on the Transformer encoder. Thereby, we have a prediction output per sequence element. If we would need a classifier over the whole sequence, the common approach is to add an additional `[CLS]` token to the sequence, representing the classifier token. However, here we focus on tasks where we have an output per element. \n",
        "\n",
        "Additionally to the Transformer architecture, we add a small input network (maps input dimensions to model dimensions), the positional encoding, and an output network (transforms output encodings to predictions). We also add the learning rate scheduler, which takes a step each iteration instead of once per epoch. This is needed for the warmup and the smooth cosine decay. The training, validation, and test step is left empty for now and will be filled for our task-specific models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5IF838PMys9"
      },
      "source": [
        "# reminder\n",
        "# super().__init__() -> https://realpython.com/python-super/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VI-M2K6RWzv"
      },
      "source": [
        "# Run This\n",
        "Original Transformer Model Code "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QomLbwRfzPDi"
      },
      "source": [
        "class TransformerPredictor(pl.LightningModule):\n",
        "    \n",
        "    \n",
        "    def __init__(self, input_dim, model_dim, num_classes, num_heads, num_layers, lr, warmup, max_iters, dropout=0.0, input_dropout=0.0):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            input_dim - Hidden dimensionality of the input\n",
        "            model_dim - Hidden dimensionality to use inside the Transformer\n",
        "            num_classes - Number of classes to predict per sequence element\n",
        "            num_heads - Number of heads to use in the Multi-Head Attention blocks\n",
        "            num_layers - Number of encoder blocks to use.\n",
        "            lr - Learning rate in the optimizer\n",
        "            warmup - Number of warmup steps. Usually between 50 and 500\n",
        "            max_iters - Number of maximum iterations the model is trained for. This is needed for the CosineWarmup scheduler\n",
        "            dropout - Dropout to apply inside the model\n",
        "            input_dropout - Dropout to apply on the input features\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self._create_model()\n",
        "        \n",
        "    \n",
        "    def _create_model(self):\n",
        "        # Input dim -> Model dim\n",
        "        self.input_net = nn.Sequential(\n",
        "            nn.Dropout(self.hparams.input_dropout),\n",
        "            nn.Linear(self.hparams.input_dim, self.hparams.model_dim)\n",
        "        )\n",
        "        # Positional encoding for sequences\n",
        "        self.positional_encoding = PositionalEncoding(d_model=self.hparams.model_dim)\n",
        "\n",
        "        # Transformer\n",
        "        self.transformer = TransformerEncoder(num_layers=self.hparams.num_layers,\n",
        "                                              input_dim=self.hparams.model_dim,\n",
        "                                              dim_feedforward=2*self.hparams.model_dim,\n",
        "                                              num_heads=self.hparams.num_heads,\n",
        "                                              dropout=self.hparams.dropout)\n",
        "        # Output classifier per sequence lement\n",
        "        self.output_net = nn.Sequential(\n",
        "            nn.Linear(self.hparams.model_dim, self.hparams.model_dim),\n",
        "            nn.LayerNorm(self.hparams.model_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(self.hparams.dropout),\n",
        "            nn.Linear(self.hparams.model_dim, self.hparams.num_classes)\n",
        "        ) \n",
        "        \n",
        "    \n",
        "    def forward(self, x, mask=None, add_positional_encoding=True):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            x - Input features of shape [Batch, SeqLen, input_dim]\n",
        "            mask - Mask to apply on the attention outputs (optional)\n",
        "            add_positional_encoding - If True, we add the positional encoding to the input.\n",
        "                                      Might not be desired for some tasks.\n",
        "        \"\"\"\n",
        "        x = self.input_net(x)\n",
        "        if add_positional_encoding:\n",
        "            x = self.positional_encoding(x)\n",
        "        x = self.transformer(x, mask=mask)\n",
        "        x = self.output_net(x)\n",
        "        return x\n",
        "    \n",
        "    \n",
        "    @torch.no_grad()\n",
        "    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n",
        "        \"\"\"\n",
        "        Function for extracting the attention matrices of the whole Transformer for a single batch.\n",
        "        Input arguments same as the forward pass.\n",
        "        \"\"\"\n",
        "        x = self.input_net(x)\n",
        "        if add_positional_encoding:\n",
        "            x = self.positional_encoding(x)\n",
        "        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n",
        "        return attention_maps\n",
        "    \n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
        "        \n",
        "        # We don't return the lr scheduler because we need to apply it per iteration, not per epoch\n",
        "        self.lr_scheduler = CosineWarmupScheduler(optimizer, \n",
        "                                                  warmup=self.hparams.warmup, \n",
        "                                                  max_iters=self.hparams.max_iters)\n",
        "        return optimizer\n",
        "    \n",
        "    \n",
        "    def optimizer_step(self, *args, **kwargs):\n",
        "        super().optimizer_step(*args, **kwargs)\n",
        "        self.lr_scheduler.step() # Step per iteration\n",
        "    \n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        raise NotImplementedError    \n",
        "    \n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        raise NotImplementedError   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o-aw7ZAzPDi"
      },
      "source": [
        "## Experiments\n",
        "\n",
        "After having finished the implementation of the Transformer architecture, we can start experimenting and apply it to various tasks. In this notebook, we will focus on two tasks: parallel Sequence-to-Sequence, and set anomaly detection. The two tasks focus on different properties of the Transformer architecture, and we go through them below.\n",
        "\n",
        "### TOY - Sequence to Sequence\n",
        "\n",
        "A Sequence-to-Sequence task represents a task where the input _and_ the output is a sequence, not necessarily of the same length. Popular tasks in this domain include machine translation and summarization. For this, we usually have a Transformer encoder for interpreting the input sequence, and a decoder for generating the output in an autoregressive manner. Here, however, we will go back to a much simpler example task and use only the encoder. Given a sequence of $N$ numbers between $0$ and $M$, the task is to reverse the input sequence. In Numpy notation, if our input is $x$, the output should be $x$[::-1]. Although this task sounds very simple, RNNs can have issues with such because the task requires long-term dependencies. Transformers are built to support such, and hence, we expect it to perform very well. \n",
        "\n",
        "First, let's create a dataset class below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gnW6-GpzPDi"
      },
      "source": [
        "class ReverseDataset(data.Dataset):\n",
        "    \n",
        "    \n",
        "    def __init__(self, num_categories, seq_len, size):\n",
        "        super().__init__()\n",
        "        self.num_categories = num_categories\n",
        "        self.seq_len = seq_len\n",
        "        self.size = size\n",
        "        # generate random syequences of int numbers\n",
        "        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n",
        "\n",
        "        self.p = True\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        inp_data = self.data[idx]\n",
        "        labels = torch.flip(inp_data, dims=(0,))\n",
        "        if self.p:\n",
        "            print(idx, type(inp_data), len(inp_data), type(labels), len(labels))\n",
        "            self.p = False\n",
        "        return inp_data, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WRmKQYazPDi"
      },
      "source": [
        "We create an arbitrary number of random sequences of numbers between 0 and `num_categories-1`. The label is simply the tensor flipped over the sequence dimension. We can create the corresponding data loaders below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tpOkP8JCAkX"
      },
      "source": [
        "# learn: \n",
        "# https://docs.python.org/3/library/functools.html -> functools.partial(func, /, *args, **keywords)\n",
        "\n",
        "# torch.randint -> https://pytorch.org/docs/stable/generated/torch.randint.html\n",
        "print(torch.randint(5, size=(3, 5)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dCcvfiUzPDj"
      },
      "source": [
        "dataset = partial(ReverseDataset, 20, 32)\n",
        "train_loader = data.DataLoader(dataset(50000), batch_size=128, shuffle=True, drop_last=True, pin_memory=True)\n",
        "val_loader   = data.DataLoader(dataset(1000), batch_size=128)\n",
        "test_loader  = data.DataLoader(dataset(10000), batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iyy2lB7zPDj"
      },
      "source": [
        "Let's look at an arbitrary sample of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-4DdF0zzPDj",
        "outputId": "fda22f72-3e47-4d94-b7e6-97f9e8ea5dd3"
      },
      "source": [
        "inp_data, labels = train_loader.dataset[0]\n",
        "print(\"Input data:\", inp_data)\n",
        "print(\"Labels:    \", labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 <class 'torch.Tensor'> 32 <class 'torch.Tensor'> 32\n",
            "Input data: tensor([ 2,  7, 16, 14,  6, 15,  0,  4, 10, 13, 18, 14, 10, 14, 11, 12, 15, 15,\n",
            "         7,  6,  9, 16, 13, 11, 19, 13,  1,  9,  7, 19, 12,  0])\n",
            "Labels:     tensor([ 0, 12, 19,  7,  9,  1, 13, 19, 11, 13, 16,  9,  6,  7, 15, 15, 12, 11,\n",
            "        14, 10, 14, 18, 13, 10,  4,  0, 15,  6, 14, 16,  7,  2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AbM8fNizPDj"
      },
      "source": [
        "During training, we pass the input sequence through the Transformer encoder and predict the output for each input token. We use the standard Cross-Entropy loss to perform this. Every number is represented as a one-hot vector. Remember that representing the categories as single scalars decreases the expressiveness of the model extremely as $0$ and $1$ are not closer related than $0$ and $9$ in our example. An alternative to a one-hot vector is using a learned embedding vector as it is provided by the PyTorch module `nn.Embedding`. However, using a one-hot vector with an additional linear layer as in our case has the same effect as an embedding layer (`self.input_net` maps one-hot vector to a dense vector, where each row of the weight matrix represents the embedding for a specific category).\n",
        "\n",
        "To implement the training dynamic, we create a new class inheriting from `TransformerPredictor` and overwriting the training, validation and test step functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFwpcyEQkHHm"
      },
      "source": [
        "# Learn\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html#torch.nn.functional.cross_entropy\n",
        "\n",
        "torch.nn.functional.cross_entropy(input, target, weight=None, size_average=None, **ignore_index=-100**, reduce=None, reduction='mean')\n",
        "\n",
        "ignore_index (int, optional) – Specifies a target value that is ignored and does not contribute to the input gradient. When size_average is True, the loss is averaged over non-ignored targets. **Default: -100**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHuqBYEkzPDj"
      },
      "source": [
        "class ReversePredictor(TransformerPredictor):\n",
        "    \n",
        "    def _calculate_loss(self, batch, mode=\"train\"):\n",
        "        # Fetch data and transform categories to one-hot vectors\n",
        "        inp_data, labels = batch\n",
        "        inp_data = F.one_hot(inp_data, num_classes=self.hparams.num_classes).float()\n",
        "        \n",
        "        # Perform prediction and calculate loss and accuracy\n",
        "        preds = self.forward(inp_data, add_positional_encoding=True)\n",
        "        loss = F.cross_entropy(preds.view(-1,preds.size(-1)), labels.view(-1))\n",
        "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
        "        \n",
        "        # Logging\n",
        "        self.log(\"%s_loss\" % mode, loss)\n",
        "        self.log(\"%s_acc\" % mode, acc)\n",
        "        return loss, acc\n",
        "        \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, _ = self._calculate_loss(batch, mode=\"train\")\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        _ = self._calculate_loss(batch, mode=\"val\")\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        _ = self._calculate_loss(batch, mode=\"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp6-wfeWzPDk"
      },
      "source": [
        "Finally, we can create a training function similar to the one we have seen in Tutorial 5 for PyTorch Lightning. We create a `pl.Trainer` object, running for $N$ epochs, logging in TensorBoard, and saving our best model based on the validation. Afterward, we test our models on the test set. An additional parameter we pass to the trainer here is `gradient_clip_val`. This clips the norm of the gradients for all parameters before taking an optimizer step and prevents the model from diverging if we obtain very high gradients at, for instance, sharp loss surfaces (see many good blog posts on gradient clipping, like [DeepAI glossary](https://deepai.org/machine-learning-glossary-and-terms/gradient-clipping)). For Transformers, gradient clipping can help to further stabilize the training during the first few iterations, and also afterward. In plain PyTorch, you can apply gradient clipping via `torch.nn.utils.clip_grad_norm_(...)` (see [documentation](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html#torch.nn.utils.clip_grad_norm_)). The clip value is usually between 0.5 and 10, depending on how harsh you want to clip large gradients. After having explained this, let's implement the training function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfgKPJ6_zPDk"
      },
      "source": [
        "def train_reverse(**kwargs):\n",
        "    # Create a PyTorch Lightning trainer with the generation callback\n",
        "    root_dir = os.path.join(CHECKPOINT_PATH, \"ReverseTask\")\n",
        "    os.makedirs(root_dir, exist_ok=True)\n",
        "    trainer = pl.Trainer(default_root_dir=root_dir, \n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
        "                         gpus=1 if str(device).startswith(\"cuda\") else 0, \n",
        "                         max_epochs=10,\n",
        "                         gradient_clip_val=5,\n",
        "                         progress_bar_refresh_rate=1)\n",
        "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
        "    \n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"ReverseTask.ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(\"Found pretrained model, loading...\")\n",
        "        model = ReversePredictor.load_from_checkpoint(pretrained_filename)\n",
        "    else:\n",
        "        model = ReversePredictor(max_iters=trainer.max_epochs*len(train_loader), **kwargs)\n",
        "        trainer.fit(model, train_loader, val_loader)\n",
        "        \n",
        "    # Test best model on validation and test set\n",
        "    val_result = trainer.test(model, test_dataloaders=val_loader, verbose=False)\n",
        "    test_result = trainer.test(model, test_dataloaders=test_loader, verbose=False)\n",
        "    result = {\"test_acc\": test_result[0][\"test_acc\"], \"val_acc\": val_result[0][\"test_acc\"]}\n",
        "    \n",
        "    model = model.to(device)\n",
        "    return model, result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUZk1IilzPDk"
      },
      "source": [
        "Finally, we can train the model. In this setup, we will use a single encoder block and a single head in the Multi-Head Attention. This is chosen because of the simplicity of the task, and in this case, the attention can actually be interpreted as an \"explanation\" of the predictions (compared to the other papers above dealing with deep Transformers). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "affa01b2a0d24a379b741dbd31a106d9",
            "64beaa33af10450bb793d23f5060693e",
            "eccb9582be6a4be2aa8620a8bdffb30c",
            "6d7fc6c6a3fb4081a415810502e086da",
            "51ebe70eda414907b93dd078364999d7",
            "f50a69a294e648259489265faa626e3a",
            "c1a0faac24574813895fab59a847b928",
            "64e2b56e1cb4498087eab282b124bf5b",
            "6ec68fb804264a4c8f329439ce8c82fa",
            "feddd3cebeec4343af71b038a1e920f5",
            "a03a85a305334f938f2af794db87d5b7",
            "e0c2826108cf410295b1c4cd861f0d0a",
            "d2472433e0474315b5701eec6117b392",
            "428bd73bf32f4714862a0ddd5053ce81",
            "8c39933691eb4d55987256ad3defadb1",
            "347a6df9d6b9420cb45f9e8cc02dabfc",
            "bd2958090d6b40b981aea2548a3cb953",
            "14e602f71f774718b70c7a2dde4f05f0",
            "578900e4887b49f9a5e3e51cd25921b9",
            "9d428ed928e04cfa8bf74925b82f8fee",
            "016cc48a766d4d8593d5de28824d9280",
            "e029382a284e4f92b65203936d58e08c",
            "e641c4b20321405fb7b494cd8d08ddf0",
            "9dc194d6037e4962806f29f3cc41a9bf",
            "1e00aad8be3541c28d41ef47607ffbe4",
            "0fd50de47c62433784d087708228f074",
            "e3c757273ff34cb19e05f3544b891260",
            "bedda42b213446d69a73f593f064dd3a",
            "94a43a8c83d94d9b9f709db3bff3e199",
            "9e9bb24b8a334ef7b8c6a02cb5a50aa1",
            "2efbaff77a354fae91c73e7eb6a9005c",
            "d61932e5633147fe8a087770872731ff"
          ]
        },
        "id": "KEVDXY36zPDk",
        "outputId": "1599c802-eae9-4e45-bb1d-040082a54a8f"
      },
      "source": [
        "CHECKPOINT_PATH=''\n",
        "reverse_model, reverse_result = train_reverse(input_dim=train_loader.dataset.num_categories,\n",
        "                                              model_dim=32,\n",
        "                                              num_heads=1,\n",
        "                                              num_classes=train_loader.dataset.num_categories,\n",
        "                                              num_layers=1,\n",
        "                                              dropout=0.0,\n",
        "                                              input_dropout=0.0,\n",
        "                                              lr=5e-4,\n",
        "                                              warmup=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\n",
            "  | Name                | Type               | Params\n",
            "-----------------------------------------------------------\n",
            "0 | input_net           | Sequential         | 672   \n",
            "1 | positional_encoding | PositionalEncoding | 0     \n",
            "2 | transformer         | TransformerEncoder | 8.5 K \n",
            "3 | output_net          | Sequential         | 1.8 K \n",
            "-----------------------------------------------------------\n",
            "11.0 K    Trainable params\n",
            "0         Non-trainable params\n",
            "11.0 K    Total params\n",
            "0.044     Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "affa01b2a0d24a379b741dbd31a106d9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Global seed set to 42\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 32, 32])\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ec68fb804264a4c8f329439ce8c82fa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  warnings.warn(*args, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd2958090d6b40b981aea2548a3cb953",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([104, 32, 32])\n",
            "Tzur DBG1 batch_size=104 seq_length=32 embed_dim=32\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e00aad8be3541c28d41ef47607ffbe4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([128, 32, 32])\n",
            "Tzur DBG1 batch_size=128 seq_length=32 embed_dim=32\n",
            "Tzur DBG1 x.size()=torch.Size([16, 32, 32])\n",
            "Tzur DBG1 batch_size=16 seq_length=32 embed_dim=32\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0M1gWhMzPDk"
      },
      "source": [
        "The warning of PyTorch Lightning regarding the number of workers can be ignored for now. As the data set is so simple and the `__getitem__` finishes a neglectable time, we don't need subprocesses to provide us the data (in fact, more workers can slow down the training as we have communication overhead among processes/threads). First, let's print the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb5Is_4fzPDl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a640b7f7-ef8c-4bad-de31-70ae529c77cd"
      },
      "source": [
        "print(\"Val accuracy:  %4.2f%%\" % (100.0 * reverse_result[\"val_acc\"]))\n",
        "print(\"Test accuracy: %4.2f%%\" % (100.0 * reverse_result[\"test_acc\"]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Val accuracy:  100.00%\n",
            "Test accuracy: 100.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RDq-tuRzPDl"
      },
      "source": [
        "As we would have expected, the Transformer can correctly solve the task. However, how does the attention in the Multi-Head Attention block looks like for an arbitrary input? Let's try to visualize it below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTJbKGDDzPDl"
      },
      "source": [
        "data_input, labels = next(iter(val_loader))\n",
        "inp_data = F.one_hot(data_input, num_classes=reverse_model.hparams.num_classes).float()\n",
        "inp_data = inp_data.to(device)\n",
        "attention_maps = reverse_model.get_attention_maps(inp_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ2l7U_DzPDl"
      },
      "source": [
        "The object `attention_maps` is a list of length $N$ where $N$ is the number of layers. Each element is a tensor of shape [Batch, Heads, SeqLen, SeqLen], which we can verify below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-GYEHYDzPDl"
      },
      "source": [
        "attention_maps[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXrORU4WzPDm"
      },
      "source": [
        "Next, we will write a plotting function that takes as input the sequences, attention maps, and an index indicating for which batch element we want to visualize the attention map. We will create a plot where over rows, we have different layers, while over columns, we show the different heads. Remember that the softmax has been applied for each row separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Pt23_4KzPDm"
      },
      "source": [
        "def plot_attention_maps(input_data, attn_maps, idx=0):\n",
        "    if input_data is not None:\n",
        "        input_data = input_data[idx].detach().cpu().numpy()\n",
        "    else:\n",
        "        input_data = np.arange(attn_maps[0][idx].shape[-1])\n",
        "    attn_maps = [m[idx].detach().cpu().numpy() for m in attn_maps]\n",
        "    \n",
        "    num_heads = attn_maps[0].shape[0]\n",
        "    num_layers = len(attn_maps)\n",
        "    seq_len = input_data.shape[0]\n",
        "    fig_size = 4 if num_heads == 1 else 3\n",
        "    fig, ax = plt.subplots(num_layers, num_heads, figsize=(num_heads*fig_size, num_layers*fig_size))\n",
        "    if num_layers == 1:\n",
        "        ax = [ax]\n",
        "    if num_heads == 1:\n",
        "        ax = [[a] for a in ax]\n",
        "    for row in range(num_layers):\n",
        "        for column in range(num_heads):\n",
        "            ax[row][column].imshow(attn_maps[row][column], origin='lower', vmin=0)\n",
        "            ax[row][column].set_xticks(list(range(seq_len)))\n",
        "            ax[row][column].set_xticklabels(input_data.tolist())\n",
        "            ax[row][column].set_yticks(list(range(seq_len)))\n",
        "            ax[row][column].set_yticklabels(input_data.tolist())\n",
        "            ax[row][column].set_title(\"Layer %i, Head %i\" % (row+1, column+1))\n",
        "    fig.subplots_adjust(hspace=0.5)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWVt82oezPDm"
      },
      "source": [
        "Finally, we can plot the attention map of our trained Transformer on the reverse task:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSUwFHxRzPDm"
      },
      "source": [
        "plot_attention_maps(data_input, attention_maps, idx=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdwqFK5AzPDm"
      },
      "source": [
        "The model has learned to attend to the token that is on the flipped index of itself. Hence, it actually does what we intended it to do. We see that it however also pays some attention to values close to the flipped index. This is because the model doesn't need the perfect, hard attention to solve this problem, but is fine with this approximate, noisy attention map. The close-by indices are caused by the similarity of the positional encoding, which we also intended with the positional encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz1GYgEZUM1J"
      },
      "source": [
        "# Tzur Language Model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd0t_wMpYfhM"
      },
      "source": [
        "### access to my own google drive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmBSozGFYd9v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fa15e3a-b30f-4aca-9d8a-0f9d0e79290b"
      },
      "source": [
        "# access to local google drive\n",
        "# source: https://colab.research.google.com/notebooks/io.ipynb#scrollTo=RWSJpsyKqHjH\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoZ0ENCuY9MT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "631532d3-5e81-4c31-881d-15d2c13ff443"
      },
      "source": [
        "# check if text exist\n",
        "DATA_PATH1='/content/drive/My Drive/Colab_files/my-LM_transformer'\n",
        "train_text_file = os.path.join(DATA_PATH1, \"english.txt\")\n",
        "print(train_text_file, os.path.isfile(train_text_file))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab_files/my-LM_transformer/english.txt True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNRcj9HQKH3T"
      },
      "source": [
        "Load english text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7QWzRg9awEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f858af3-5432-46a4-c4d9-b6e27ea06237"
      },
      "source": [
        "src_data = open(train_text_file).read().strip().split('\\n')\n",
        "print(type(src_data), len(src_data))\n",
        "#print(src_data[:3], src_data[-3:])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> 154883\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5awJhimLeefB"
      },
      "source": [
        "## Tokenizing the text using spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKi1yRtFe0UG"
      },
      "source": [
        "# src: https://github.com/SamLynnEvans/Transformer/blob/master/Tokenize.py\n",
        "import spacy\n",
        "import re\n",
        "#import torchtext\n",
        "#from torchtext import data\n",
        "\n",
        "class tokenize(object):\n",
        "    ''' \n",
        "    self defined start of sentence and end of sentence \n",
        "    '''    \n",
        "    def __init__(self, lang, init_token='SOS', eos_token='EOS'):\n",
        "        self.nlp = spacy.load(lang)\n",
        "        self.initToken = init_token\n",
        "        self.eosToken  = eos_token\n",
        "            \n",
        "    def tokenizer(self, sentence):\n",
        "        sentence = re.sub(\n",
        "        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n",
        "        sentence = re.sub(r\"[ ]+\", \" \", sentence)\n",
        "        sentence = re.sub(r\"\\!+\", \"!\", sentence)\n",
        "        sentence = re.sub(r\"\\,+\", \",\", sentence)\n",
        "        sentence = re.sub(r\"\\?+\", \"?\", sentence)\n",
        "        sentence = sentence.lower()\n",
        "        tokens = [tok.text for tok in self.nlp.tokenizer(sentence) if tok.text != \" \"]\n",
        "        return [init_token] + tokens + [eos_token]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L52guAredO8w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c26e654-f8ae-420a-cb5f-93c596adaf9f"
      },
      "source": [
        "# source https://github.com/SamLynnEvans/Transformer/blob/e06ae2810f119c75aa34585442872026875e6462/Process.py#L25\n",
        "\n",
        "print(\"loading spacy tokenizer ...\")\n",
        "en_tokenizer = tokenize('en')\n",
        "\n",
        "\n",
        "init_token='SOS'\n",
        "eos_token='EOS'\n",
        "#pad_token='<pad>'\n",
        "\n",
        "for line in src_data[74500:74505]:\n",
        "    print(en_tokenizer.tokenizer(line))\n",
        "#SRC = data.Field(lower=True, tokenize=en_tokenizer)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading spacy tokenizer ...\n",
            "['SOS', 'if', 'you', 'do', \"n't\", 'know', ',', 'who', 'does', '?', 'EOS']\n",
            "['SOS', 'in', 'my', 'opinion', ',', 'you', \"'re\", 'wrong', '.', 'EOS']\n",
            "['SOS', 'in', 'that', 'case', ',', 'you', 'are', 'right', '.', 'EOS']\n",
            "['SOS', 'in', 'the', 'end', ',', 'he', 'did', 'not', 'come', '.', 'EOS']\n",
            "['SOS', 'in', 'what', 'month', 'were', 'you', 'born', '?', 'EOS']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQABBeTWpF9F",
        "outputId": "40974826-3676-4f88-e2ec-5066206b8fd4"
      },
      "source": [
        "src_data = src_data[:256]\n",
        "print(src_data[-5:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['How deep?', 'How nice!', 'How nice!', 'How nice!', 'How nice!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMHjzfL6RhAA",
        "outputId": "f071d60a-e709-4d50-de37-a420f9a66932"
      },
      "source": [
        "\n",
        "# tokenize all loaded text and creat vocabulary\n",
        "vocab = set()\n",
        "maxSentenceLen = 0\n",
        "for index, sentence in enumerate(src_data):\n",
        "    #print(sentence)\n",
        "    TokenizedSentence1 = en_tokenizer.tokenizer(sentence)\n",
        "    maxSentenceLen = max(maxSentenceLen, len(TokenizedSentence1))\n",
        "    if index%20==0:\n",
        "        print(index, sentence, TokenizedSentence1)\n",
        "\n",
        "    doc = nlp(' '.join(TokenizedSentence1))\n",
        "    #print(len(doc), len(TokenizedSentence1))\n",
        "    for ii, doc_token in enumerate(doc):\n",
        "        vocab.add(doc_token.text)\n",
        "    #    print(ii, doc_token)\n",
        "    #for ii, token in enumerate(TokenizedSentence1):\n",
        "        #    print(\"* \",ii, token) \n",
        "        #vocab.add(token)\n",
        "\n",
        "#print(dictionary['SOS'])\n",
        "#idx=147\n",
        "#print(src_data[idx])\n",
        "#tokenizedSentence = en_tokenizer.tokenizer(src_data[idx])\n",
        "#print(tokenizedSentence)\n",
        "#doc = nlp(' '.join(tokenizedSentence))\n",
        "\n",
        "#for index, token in enumerate(doc):\n",
        "#    print(f\"!{token.text}!\")\n",
        "#    print(dictionary[token.text])\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Vocab size: {len(vocab)}\")\n",
        "dictionary = dict()\n",
        "for index, token in enumerate(vocab):\n",
        "    dictionary[token] = index\n",
        "\n",
        "print(dictionary)\n",
        "w='SOS'\n",
        "print(f\"{w}: {dictionary[w]}\")\n",
        "w='go'\n",
        "print(f\"{w}: {dictionary[w]}\")\n",
        "\n",
        "print(f\"maxSentenceLen = {maxSentenceLen}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Go. ['SOS', 'go', '.', 'EOS']\n",
            "20 Attack! ['SOS', 'attack', 'EOS']\n",
            "40 I know. ['SOS', 'i', 'know', '.', 'EOS']\n",
            "60 Thanks. ['SOS', 'thanks', '.', 'EOS']\n",
            "80 Be nice. ['SOS', 'be', 'nice', '.', 'EOS']\n",
            "100 Drop it! ['SOS', 'drop', 'it', 'EOS']\n",
            "120 Hang on! ['SOS', 'hang', 'on', 'EOS']\n",
            "140 I'll go. ['SOS', 'i', \"'ll\", 'go', '.', 'EOS']\n",
            "160 Open up. ['SOS', 'open', 'up', '.', 'EOS']\n",
            "180 Wake up. ['SOS', 'wake', 'up', '.', 'EOS']\n",
            "200 Ask them. ['SOS', 'ask', 'them', '.', 'EOS']\n",
            "220 Drive on. ['SOS', 'drive', 'on', '.', 'EOS']\n",
            "240 Good job! ['SOS', 'good', 'job', 'EOS']\n",
            "Vocab size: 120\n",
            "{'tried': 0, 'cheers': 1, 'wash': 2, 'wake': 3, 'you': 4, 'guys': 5, 'up': 6, 'off': 7, 'deep': 8, '.': 9, 'jump': 10, 'fire': 11, 'back': 12, 'real': 13, 'help': 14, 'now': 15, 'open': 16, 'job': 17, 'fair': 18, 'me': 19, 'hop': 20, 'on': 21, 'sad': 22, 'show': 23, 'm': 24, 'keep': 25, 'who': 26, 'get': 27, 'hug': 28, 'too': 29, 'am': 30, 'go': 31, \"'\": 32, 'be': 33, 'long': 34, 'him': 35, 'hi': 36, ',': 37, 'calm': 38, 'EOS': 39, 'fat': 40, 'way': 41, 'in': 42, 'won': 43, 'find': 44, 'man': 45, 'drop': 46, 'know': 47, 'tell': 48, 'have': 49, 'thanks': 50, 'tom': 51, 'cuff': 52, 'still': 53, 'we': 54, 'hit': 55, 'tries': 56, 'away': 57, 'how': 58, 'fun': 59, 'goodbye': 60, 'SOS': 61, 'agree': 62, 'beats': 63, 'drive': 64, 'hold': 65, 'listen': 66, '19': 67, \"'ll\": 68, 'so': 69, 'call': 70, 'kiss': 71, 'nice': 72, 'hang': 73, 'ill': 74, \"'s\": 75, 'wait': 76, 'oh': 77, 'really': 78, 'fit': 79, 'come': 80, 'run': 81, 'grab': 82, 'out': 83, 'it': 84, 'attack': 85, 'got': 86, 'beat': 87, '?': 88, 'kind': 89, 'try': 90, 'runs': 91, 'ahead': 92, 'fell': 93, 'join': 94, 'a': 95, 'he': 96, 'ask': 97, 'shy': 98, 'good': 99, 'us': 100, 'perfect': 101, 'ok': 102, 'them': 103, 'stop': 104, 'quit': 105, 'cute': 106, 'slow': 107, 'no': 108, 'take': 109, 'see': 110, 'lost': 111, 'wet': 112, 'cheer': 113, 'awesome': 114, 'i': 115, 'shut': 116, 'down': 117, 'cool': 118, 'left': 119}\n",
            "SOS: 61\n",
            "go: 31\n",
            "maxSentenceLen = 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBsEF9qdYgG4"
      },
      "source": [
        "how to create one-hot vectors as output lables?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs8cLTMiYo0j"
      },
      "source": [
        "# not what i'm looking for !!\n",
        "import torch\n",
        "y = torch.eye(4) \n",
        "print(y)\n",
        "print(y[torch.arange(0,2)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDR8_8HNZfv7",
        "outputId": "75f62c0f-9485-449e-d48d-35283e83e34e"
      },
      "source": [
        "#import torch\n",
        "#from torch.nn import functional\n",
        "#label = torch.tensor([2])\n",
        "num_class = 5\n",
        "for label in [0, 1, 4]:\n",
        "    label2one_hot = F.one_hot(torch.tensor([label]), num_classes=num_class)\n",
        "    print(f\"longTensor {label2one_hot}\")\n",
        "    print(f\"ndarray {label2one_hot.numpy()}\")\n",
        "    print(f\"list: {label2one_hot.numpy().tolist()}\")\n",
        "\n",
        "label2one_hot = F.one_hot(torch.tensor([0, 2,3]), num_classes=num_class)\n",
        "print(f\"longTensor {label2one_hot}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "longTensor tensor([[1, 0, 0, 0, 0]])\n",
            "ndarray [[1 0 0 0 0]]\n",
            "list: [[1, 0, 0, 0, 0]]\n",
            "longTensor tensor([[0, 1, 0, 0, 0]])\n",
            "ndarray [[0 1 0 0 0]]\n",
            "list: [[0, 1, 0, 0, 0]]\n",
            "longTensor tensor([[0, 0, 0, 0, 1]])\n",
            "ndarray [[0 0 0 0 1]]\n",
            "list: [[0, 0, 0, 0, 1]]\n",
            "longTensor tensor([[1, 0, 0, 0, 0],\n",
            "        [0, 0, 1, 0, 0],\n",
            "        [0, 0, 0, 1, 0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqOvJPu1JDJ_"
      },
      "source": [
        "#Todo\n",
        "add word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EcIpJunL13j"
      },
      "source": [
        "# Spacy words embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mSfeZU8ORKm",
        "outputId": "ec418958-1b2f-4836-8598-0f146664c26f"
      },
      "source": [
        "# load spacy english model\n",
        "!python -m spacy download en_core_web_md\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_md==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (57.2.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (4.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCEM48eae5M9"
      },
      "source": [
        "# learn: supported tokens list\n",
        "# https://stackoverflow.com/questions/54495502/how-to-get-all-words-from-spacy-vocab\n",
        "print(len(list(nlp.vocab.strings)))\n",
        "words = list(nlp.vocab.strings)\n",
        "\n",
        "for a in range(0,1148,20):\n",
        "    print(a, words[a:a+19])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bnk86ipmOqRx"
      },
      "source": [
        "doc = nlp(src_data[24])\n",
        "print(src_data[24], [t.orth_ for t in doc])\n",
        "print([type(t.orth_) for t in doc])\n",
        "print(f\"token '{doc[0]}' embedding {doc[0].vector} length is {len(doc[0].vector)}\")\n",
        "\n",
        "sentenceTokens = en_tokenizer.tokenizer(['tzur '+src_data[24]])\n",
        "print(sentenceTokens)\n",
        "doc1 = nlp(' '.join(sentenceTokens))\n",
        "print(doc1)\n",
        "print(src_data[24], [t for t in doc1])\n",
        "print(f\"token '{doc1[0]}' embedding {doc1[0].vector} length is {len(doc1[0].vector)}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOnHGQK_IooM"
      },
      "source": [
        "Add words vectors from word to vec and create dataset\n",
        "\n",
        "Example of loading data into Torch [data.Dataset](https://pytorch.org/docs/stable/data.html) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRdjJQpL70GP"
      },
      "source": [
        "print(type(en_tokenizer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHD_rJ-FIkVK"
      },
      "source": [
        "#https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset\n",
        "'''\n",
        "    All datasets that represent a map from keys to data samples should subclass it. \n",
        "    All subclasses should overwrite __getitem__(), supporting fetching a data sample for a given key. \n",
        "    Subclasses could also optionally overwrite __len__(), \n",
        "    which is expected to return the size of the dataset by many Sampler implementations \n",
        "    and the default options of DataLoader.\n",
        "'''\n",
        "\n",
        "class CreatDataset(data.Dataset):\n",
        "   \n",
        "    def __init__(self, num_categories, input_dim, output_max_dim, dictionary, src_data, en_tokenizer, nlp, train=True):\n",
        "        super().__init__()\n",
        "        self.num_categories = num_categories\n",
        "        self.input_dim = input_dim\n",
        "        self.output_max_dim = output_max_dim\n",
        "        self.raw_data = src_data\n",
        "        self.dictionary = dictionary\n",
        "        self.tokenizer = en_tokenizer\n",
        "        self.nlp = nlp\n",
        "        self.size = len(self.raw_data)\n",
        "        self.ignore_index = -100\n",
        "        \n",
        "        # generate random syequences of int numbers\n",
        "        #self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n",
        "        self.train = train\n",
        "\n",
        "        if not train:\n",
        "            print(\"Tzur: not train\")\n",
        "\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        under the assumption that seq_len is equal to the longest words sequence\n",
        "        '''\n",
        "            \n",
        "        print(f\"Tzur DBG: idx={idx}\")\n",
        "        tokenizedSentence = self.tokenizer.tokenizer(self.raw_data[idx])\n",
        "        doc = self.nlp(' '.join(tokenizedSentence))\n",
        "        inp_data = np.empty(0)\n",
        "        labels = list()\n",
        "        for index, token in enumerate(doc):\n",
        "            inp_data = np.append(inp_data, token.vector)\n",
        "            #labels.insert(0, self.dictionary[token.orth_])\n",
        "            labels.insert(0, self.dictionary[token.text])\n",
        "\n",
        "        inp_data = np.pad(inp_data, (0, self.input_dim-len(inp_data)), 'constant', \n",
        "                          constant_values=(0, 0)).astype(np.float32)\n",
        "        #inp_data = torch.cat(torch.tensor(inp_data), dim=0)\n",
        "        inp_data = torch.tensor(inp_data)\n",
        "\n",
        "        # not efficient, batch should be grouped and use the longest in each batch\n",
        "        labels = np.pad(labels, (0, self.output_max_dim-len(labels)), 'constant',\n",
        "                          constant_values=(0, self.ignore_index))\n",
        "        #labels = torch.cat(torch.tensor(labels), dim=0)\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "        #original code\n",
        "        #inp_data = self.data[idx]\n",
        "        #labels = torch.flip(inp_data, dims=(0,))\n",
        "        print(self.train, type(inp_data), len(inp_data), type(labels), len(labels))\n",
        "        return inp_data, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBpWUyCtqJZi",
        "outputId": "3d7bd6f8-50d1-4dc0-8de2-c9921a37ffec"
      },
      "source": [
        "print(dictionary['SOS'])\n",
        "idx=109\n",
        "print(src_data[idx])\n",
        "tokenizedSentence = en_tokenizer.tokenizer(src_data[idx])\n",
        "print(tokenizedSentence)\n",
        "doc = nlp(' '.join(tokenizedSentence))\n",
        "print(doc)\n",
        "print(len(tokenizedSentence), len(doc))\n",
        "\n",
        "for token in doc:\n",
        "    print(token, token.text)\n",
        "\n",
        "\n",
        "\n",
        "#for a, b in zip(tokenizedSentence, doc):\n",
        "#    print(a, b)\n",
        "#for index, token in enumerate(doc):\n",
        "#    print(f\"!{token.text}!\")\n",
        "#    print(dictionary[token.text])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "61\n",
            "Go away.\n",
            "['SOS', 'go', 'away', '.', 'EOS']\n",
            "SOS go away . EOS\n",
            "5 5\n",
            "SOS SOS\n",
            "go go\n",
            "away away\n",
            ". .\n",
            "EOS EOS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbnPleAJt_Ix"
      },
      "source": [
        "# exmple of padding with zeros.\n",
        "# print(torch.randint(5, size=(3, 5)))\n",
        "a= [ 1,2,3]\n",
        "c = np.empty(0)\n",
        "for b in range(3):\n",
        "    c= np.append(c, a)\n",
        "c = np.pad(c, (0, 3), 'constant', constant_values=(0,0))\n",
        "print(torch.tensor(c))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Bl23rdacZt-"
      },
      "source": [
        "create dataSet for the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r67mmDMzmCdZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b59f58eb-0afc-403d-fa23-c3a5ea7f0986"
      },
      "source": [
        "\n",
        "wordEmbeddingVecLen = 96 \n",
        "train_dataSet = CreatDataset(len(vocab), maxSentenceLen * wordEmbeddingVecLen, \n",
        "                       maxSentenceLen, dictionary, src_data, en_tokenizer, nlp, train=True)\n",
        "eval_dataSet = CreatDataset(len(vocab), maxSentenceLen * wordEmbeddingVecLen, \n",
        "                       maxSentenceLen, dictionary, src_data, en_tokenizer, nlp, train=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tzur: not train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SI3WDQK_cjOU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58cb5b31-9412-4f34-9664-48f7fa1103ec"
      },
      "source": [
        "# check the dataSet\n",
        "\n",
        "print(train_dataSet.__len__())\n",
        "\n",
        "\n",
        "indx = 0\n",
        "print(f\" Sentence: {src_data[indx]}\")\n",
        "tokonizedSentence = en_tokenizer.tokenizer(src_data[indx])\n",
        "print(f\" tokens: {tokonizedSentence}\")\n",
        "print(f\" Token indexs {[ dictionary[t] for t in tokonizedSentence ]}\")\n",
        "inp_data, labels = train_dataSet.__getitem__(0)\n",
        "print(type(inp_data[0]))\n",
        "print(labels)\n",
        "\n",
        "\n",
        "for ii in range(10,25,3):\n",
        "    print(f\" Sentence: {src_data[ii]}\")\n",
        "    inp_data, labels = train_dataSet.__getitem__(ii)\n",
        "    print(f\" Output labels {labels}\")\n",
        "    print(len(inp_data), len(labels))\n",
        "\n",
        "\n",
        "\n",
        "#train_loader = data.DataLoader(dataset(50000), batch_size=4, shuffle=False, drop_last=True, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "256\n",
            " Sentence: Go.\n",
            " tokens: ['SOS', 'go', '.', 'EOS']\n",
            " Token indexs [61, 31, 9, 39]\n",
            "Tzur DBG: idx=0\n",
            "True <class 'torch.Tensor'> 576 <class 'torch.Tensor'> 6\n",
            "<class 'torch.Tensor'>\n",
            "tensor([  39,    9,   31,   61, -100, -100])\n",
            " Sentence: Wait!\n",
            "Tzur DBG: idx=10\n",
            "True <class 'torch.Tensor'> 576 <class 'torch.Tensor'> 6\n",
            " Output labels tensor([  39,   76,   61, -100, -100, -100])\n",
            "576 6\n",
            " Sentence: Go on.\n",
            "Tzur DBG: idx=13\n",
            "True <class 'torch.Tensor'> 576 <class 'torch.Tensor'> 6\n",
            " Output labels tensor([  39,    9,   21,   31,   61, -100])\n",
            "576 6\n",
            " Sentence: I won!\n",
            "Tzur DBG: idx=16\n",
            "True <class 'torch.Tensor'> 576 <class 'torch.Tensor'> 6\n",
            " Output labels tensor([  39,   43,  115,   61, -100, -100])\n",
            "576 6\n",
            " Sentence: Attack!\n",
            "Tzur DBG: idx=19\n",
            "True <class 'torch.Tensor'> 576 <class 'torch.Tensor'> 6\n",
            " Output labels tensor([  39,   85,   61, -100, -100, -100])\n",
            "576 6\n",
            " Sentence: Cheers!\n",
            "Tzur DBG: idx=22\n",
            "True <class 'torch.Tensor'> 576 <class 'torch.Tensor'> 6\n",
            " Output labels tensor([  39,    1,   61, -100, -100, -100])\n",
            "576 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqVFck9aoWkc"
      },
      "source": [
        "a , b = dataSet[5]\n",
        "print(b)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gViwyoKUeqC8"
      },
      "source": [
        "## Set the training and avaluation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKAiEbmKsZUR"
      },
      "source": [
        "# Split train into train+val\n",
        "## skip for now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpKYwufEe7Jg"
      },
      "source": [
        "\n",
        "\n",
        "# Group corresponding image features and labels\n",
        "#train_feats, train_labels = train_set_feats[train_indices], labels[train_indices]\n",
        "#val_feats,   val_labels   = train_set_feats[val_indices],   labels[val_indices]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PSaW2moswG1"
      },
      "source": [
        "# Original - as example \n",
        "### don't run - copied example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKltRBoOe97o"
      },
      "source": [
        "# setup our datasets and data loaders \n",
        "SET_SIZE = 10\n",
        "test_labels = torch.LongTensor(test_set.targets)\n",
        "\n",
        "train_anom_dataset = SetAnomalyDataset(train_feats, train_labels, set_size=SET_SIZE, train=True)\n",
        "val_anom_dataset   = SetAnomalyDataset(val_feats,   val_labels,   set_size=SET_SIZE, train=False)\n",
        "test_anom_dataset  = SetAnomalyDataset(test_feats,  test_labels,  set_size=SET_SIZE, train=False)\n",
        "\n",
        "train_anom_loader = data.DataLoader(train_anom_dataset, batch_size=64, shuffle=True,  drop_last=True,  num_workers=2, pin_memory=True)\n",
        "val_anom_loader   = data.DataLoader(val_anom_dataset,   batch_size=64, shuffle=False, drop_last=False, num_workers=2)\n",
        "test_anom_loader  = data.DataLoader(test_anom_dataset,  batch_size=64, shuffle=False, drop_last=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfow-kCsQrj4"
      },
      "source": [
        "## Run This"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwkmbYJ0s4IA"
      },
      "source": [
        "# setup our datasets and data loaders\n",
        "train_Loader = data.DataLoader(train_dataSet, batch_size=32, shuffle=True,  drop_last=True,  num_workers=1)\n",
        "eval_Loader  = data.DataLoader(eval_dataSet, batch_size=32, shuffle=False,  drop_last=False,  num_workers=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srSGjvWGfiYq"
      },
      "source": [
        "class reverseSentencePredictor(TransformerPredictor):\n",
        "    \n",
        "    def _calculate_loss(self, batch, mode=\"train\"):\n",
        "        in_data, labels = batch\n",
        "    \n",
        "        preds = self.forward(in_data, add_positional_encoding=False) \n",
        "        preds = preds.squeeze(dim=-1) # Shape: [Batch_size, set_size]\n",
        "        loss = F.cross_entropy(preds, labels) # Softmax/CE over set dimension\n",
        "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
        "        self.log(\"%s_loss\" % mode, loss)\n",
        "        self.log(\"%s_acc\" % mode, acc, on_step=False, on_epoch=True)\n",
        "        return loss, acc\n",
        "        \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, _ = self._calculate_loss(batch, mode=\"train\")\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        print(type(batch), len(batch))\n",
        "        print(len(batch[0]), len(batch[1]))\n",
        "        _ = self._calculate_loss(batch, mode=\"val\")\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        _ = self._calculate_loss(batch, mode=\"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_KSYJQYf_Y7",
        "cellView": "code"
      },
      "source": [
        "#@title Default title text\n",
        "CHECKPOINT_PATH=DATA_PATH1\n",
        "def train_ReveresWordsPredictor(**kwargs):\n",
        "    # Create a PyTorch Lightning trainer with the generation callback\n",
        "    root_dir = os.path.join(CHECKPOINT_PATH, \"SetReverseLMPredictor\")\n",
        "    os.makedirs(root_dir, exist_ok=True)\n",
        "    trainer = pl.Trainer(default_root_dir=root_dir, \n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
        "                         gpus=1 if str(device).startswith(\"cuda\") else 0, \n",
        "                         max_epochs=10,\n",
        "                         gradient_clip_val=2,\n",
        "                         progress_bar_refresh_rate=1)\n",
        "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
        "    \n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"SetReverseLMPredictor.ckpt\")\n",
        "    #if os.path.isfile(pretrained_filename):\n",
        "    #    print(\"Found pretrained model, loading...\")\n",
        "    #    model = _?_AnomalyPredictor.load_from_checkpoint(pretrained_filename)\n",
        "    #else:\n",
        "    model = reverseSentencePredictor(max_iters=trainer.max_epochs*len(train_Loader), **kwargs)\n",
        "    trainer.fit(model, train_Loader, eval_dataSet)\n",
        "    model = reverseSentencePredictor.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
        "    \n",
        "    # Test best model on validation and test set\n",
        "    train_result = trainer.test(model, test_dataloaders=train_anom_loader, verbose=False)\n",
        "    result = {\"train_acc\": train_result[0][\"test_acc\"]}\n",
        "    #val_result = trainer.test(model, test_dataloaders=val_anom_loader, verbose=True)\n",
        "    #test_result = trainer.test(model, test_dataloaders=test_anom_loader, verbose=False)\n",
        "    #result = {\"test_acc\": test_result[0][\"test_acc\"], \"val_acc\": val_result[0][\"test_acc\"], \"train_acc\": train_result[0][\"test_acc\"]}\n",
        "    \n",
        "    model = model.to(device)\n",
        "    return model, result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4uLqiSkx0i_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7742024b-34ef-4334-8aac-1e82914f52a0"
      },
      "source": [
        "print(dir(train_Loader.dataset))\n",
        "print(train_Loader.dataset.dictionary)\n",
        "print(train_Loader.dataset.num_categories)\n",
        "print(train_Loader.dataset.input_dim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['__add__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', 'dictionary', 'ignore_index', 'input_dim', 'nlp', 'num_categories', 'output_max_dim', 'raw_data', 'size', 'tokenizer']\n",
            "{'cheers': 0, 'help': 1, 'ill': 2, 'find': 3, 'awesome': 4, 'm': 5, 'we': 6, 'cool': 7, 'hug': 8, 'wash': 9, '.': 10, 'have': 11, 'am': 12, 'call': 13, 'runs': 14, 'hit': 15, 'way': 16, 'job': 17, 'slow': 18, 'beats': 19, 'drive': 20, 'really': 21, 'agree': 22, 'no': 23, 'good': 24, 'goodbye': 25, \"'s\": 26, 'wake': 27, 'grab': 28, 'he': 29, 'quit': 30, 'ask': 31, 'too': 32, 'fair': 33, '?': 34, 'know': 35, 'tom': 36, 'try': 37, 'join': 38, 'fat': 39, 'kiss': 40, ',': 41, 'hop': 42, 'man': 43, 'SOS': 44, 'got': 45, 'cuff': 46, 'attack': 47, 'i': 48, 'long': 49, 'him': 50, 'hold': 51, 'away': 52, 'now': 53, 'deep': 54, 'wait': 55, 'get': 56, \"'ll\": 57, 'tries': 58, 'so': 59, 'nice': 60, 'hi': 61, 'fell': 62, 'drop': 63, 'keep': 64, 'fire': 65, 'up': 66, 'lost': 67, 'calm': 68, 'shy': 69, 'cheer': 70, 'go': 71, 'open': 72, 'hang': 73, 'thanks': 74, 'who': 75, 'still': 76, 'left': 77, 'fun': 78, 'tell': 79, '19': 80, 'them': 81, 'you': 82, 'cute': 83, 'come': 84, 'listen': 85, 'won': 86, 'off': 87, 'ahead': 88, 'show': 89, 'run': 90, 'be': 91, 'take': 92, 'us': 93, \"'\": 94, 'guys': 95, 'down': 96, 'me': 97, 'beat': 98, 'jump': 99, 'fit': 100, 'tried': 101, 'wet': 102, 'shut': 103, 'see': 104, 'EOS': 105, 'in': 106, 'it': 107, 'ok': 108, 'a': 109, 'real': 110, 'oh': 111, 'kind': 112, 'back': 113, 'perfect': 114, 'sad': 115, 'how': 116, 'on': 117, 'out': 118, 'stop': 119}\n",
            "120\n",
            "576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsYhURecgWWB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688,
          "referenced_widgets": [
            "a6e70f11baa54e3da61ba4c3701bda21",
            "50941ae1b92f4f96a34d560f0a74af5b",
            "9bc60ab01ff845ae9bef159a89b3cbce",
            "f7b8f7d424a445fb981d4672144c5bbb",
            "6cb09a617367447f9748f1ae20193544",
            "724a8c5f72fe4890a9e1b31cff867122",
            "fadc8f933dbb468f9d0e72d8ddda167f",
            "e4f2ea2f75a04e05849e30b5726c231e"
          ]
        },
        "outputId": "5dfd4714-183e-4c5a-8e09-63d00cc1ea42"
      },
      "source": [
        "LMreverse_model, LMreverse_result = train_ReveresWordsPredictor(input_dim=train_Loader.dataset.input_dim,\n",
        "                                              model_dim=32,\n",
        "                                              num_heads=1,\n",
        "                                              num_classes=train_Loader.dataset.num_categories,\n",
        "                                              num_layers=4,\n",
        "                                              dropout=0.1,\n",
        "                                              input_dropout=0.0,\n",
        "                                              lr=5e-4,\n",
        "                                              warmup=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "\n",
            "  | Name                | Type               | Params\n",
            "-----------------------------------------------------------\n",
            "0 | input_net           | Sequential         | 18.5 K\n",
            "1 | positional_encoding | PositionalEncoding | 0     \n",
            "2 | transformer         | TransformerEncoder | 34.2 K\n",
            "3 | output_net          | Sequential         | 5.1 K \n",
            "-----------------------------------------------------------\n",
            "57.7 K    Trainable params\n",
            "0         Non-trainable params\n",
            "57.7 K    Total params\n",
            "0.231     Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6e70f11baa54e3da61ba4c3701bda21",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Tzur DBG: idx=0\n",
            "False <class 'torch.Tensor'> 576 <class 'torch.Tensor'> 6\n",
            "<class 'tuple'> 2\n",
            "576 6\n",
            "Tzur DBG1 x.size()=torch.Size([32])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-de03799232cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                                               \u001b[0minput_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                               \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                                               warmup=100)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-fcdb9e875614>\u001b[0m in \u001b[0;36mtrain_ReveresWordsPredictor\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreverseSentencePredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_Loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataSet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreverseSentencePredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    456\u001b[0m         )\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    795\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'pl.Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'pl.Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'pl.Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# double dispatch to initiate the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'pl.Trainer'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogress_bar_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_sanity_check\u001b[0;34m(self, ref_model)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;31m# run eval step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_sanity_check_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_evaluation\u001b[0;34m(self, on_epoch)\u001b[0m\n\u001b[1;32m    960\u001b[0m                 \u001b[0;31m# lightning module methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"evaluation_step_and_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    963\u001b[0m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/evaluation_loop.py\u001b[0m in \u001b[0;36mevaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mmodel_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_fx_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"validation_step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;31m# capture any logged information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_step_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSTEP_OUTPUT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-8b4bca5e1541>\u001b[0m in \u001b[0;36mvalidation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-8b4bca5e1541>\u001b[0m in \u001b[0;36m_calculate_loss\u001b[0;34m(self, batch, mode)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0min_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_positional_encoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Shape: [Batch_size, set_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Softmax/CE over set dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-27858e833c81>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask, add_positional_encoding)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_positional_encoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpositional_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-d76cb3017769>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-1191de8d5eff>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Attention part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mattn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-693c088bf5b2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask, return_attention)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_attention\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Tzur DBG1 x.size()={x.size()}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Tzur DBG1 batch_size={batch_size} seq_length={seq_length} embed_dim={embed_dim}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mqkv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqkv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W49HMcq6gfPB"
      },
      "source": [
        "print(\"Train accuracy: %4.2f%%\" % (100.0*anomaly_result[\"train_acc\"]))\n",
        "print(\"Val accuracy:   %4.2f%%\" % (100.0*anomaly_result[\"val_acc\"]))\n",
        "print(\"Test accuracy:  %4.2f%%\" % (100.0*anomaly_result[\"test_acc\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DDLcRTEzPDn"
      },
      "source": [
        "### Set Anomaly Detection\n",
        "\n",
        "Besides sequences, sets are another data structure that is relevant for many applications. In contrast to sequences, elements are unordered in a set. RNNs can only be applied on sets by assuming an order in the data, which however biases the model towards a non-existing order in the data. [Vinyals et al. (2015)](https://arxiv.org/abs/1511.06391) and other papers have shown that the assumed order can have a significant impact on the model's performance, and hence, we should try to not use RNNs on sets. Ideally, our model should be permutation-equivariant/invariant such that the output is the same no matter how we sort the elements in a set. \n",
        "\n",
        "Transformers offer the perfect architecture for this as the Multi-Head Attention is permutation-equivariant, and thus, outputs the same values no matter in what order we enter the inputs (inputs and outputs are permuted equally). The task we are looking at for sets is _Set Anomaly Detection_ which means that we try to find the element(s) in a set that does not fit the others. In the research community, the common application of anomaly detection is performed on a set of images, where $N-1$ images belong to the same category/have the same high-level features while one belongs to another category. Note that category does not necessarily have to relate to a class in a standard classification problem, but could be the combination of multiple features. For instance, on a face dataset, this could be people with glasses, male, beard, etc. An example of distinguishing different animals can be seen below. The first four images show foxes, while the last represents a different animal. We want to recognize that the last image shows a different animal, but it is not relevant which class of animal it is.\n",
        "\n",
        "<center width=\"100%\" style=\"padding:20px\"><img src=\"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial6/cifar100_example_anomaly.png?raw=1\" width=\"600px\"></center>\n",
        "\n",
        "In this tutorial, we will use the CIFAR100 dataset. CIFAR100 has 600 images for 100 classes each with a resolution of 32x32, similar to CIFAR10. The larger amount of classes requires the model to attend to specific features in the images instead of coarse features as in CIFAR10, therefore making the task harder. We will show the model a set of 9 images of one class, and 1 image from another class. The task is to find the image that is from a different class than the other images.\n",
        "Using the raw images directly as input to the Transformer is not a good idea, because it is not translation invariant as a CNN, and would need to learn to detect image features from high-dimensional input first of all. Instead, we will use a pre-trained ResNet34 model from the torchvision package to obtain high-level, low-dimensional features of the images. The ResNet model has been pre-trained on the [ImageNet](http://image-net.org/) dataset which contains 1 million images of 1k classes and varying resolutions. However, during training and testing, the images are usually scaled to a resolution of 224x224, and hence we rescale our CIFAR images to this resolution as well. Below, we will load the dataset, and prepare the data for being processed by the ResNet model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK3gUQ74zPDn"
      },
      "source": [
        "# ImageNet statistics\n",
        "DATA_MEANS = np.array([0.485, 0.456, 0.406])\n",
        "DATA_STD = np.array([0.229, 0.224, 0.225])\n",
        "# As torch tensors for later preprocessing\n",
        "TORCH_DATA_MEANS = torch.from_numpy(DATA_MEANS).view(1,3,1,1)\n",
        "TORCH_DATA_STD = torch.from_numpy(DATA_STD).view(1,3,1,1)\n",
        "\n",
        "# Resize to 224x224, and normalize to ImageNet statistic\n",
        "transform = transforms.Compose([transforms.Resize((224,224)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(DATA_MEANS, DATA_STD)\n",
        "                                ])\n",
        "# Loading the training dataset. \n",
        "train_set = CIFAR100(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
        "\n",
        "# Loading the test set\n",
        "test_set = CIFAR100(root=DATASET_PATH, train=False, transform=transform, download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaaF9PaDzPDn"
      },
      "source": [
        "Next, we want to run the pre-trained ResNet model on the images, and extract the features before the classification layer. These are the most high-level features, and should sufficiently describe the images. CIFAR100 has some similarity to ImageNet, and thus we are not retraining the ResNet model in any form. However, if you would want to get the best performance and have a very large dataset, it would be better to add the ResNet to the computation graph during training and finetune its parameters as well. As we don't have a large enough dataset and want to train our model efficiently, we will extract the features beforehand. Let's load and prepare the model below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOKuyMwGzPDn"
      },
      "source": [
        "import os\n",
        "os.environ[\"TORCH_HOME\"] = CHECKPOINT_PATH\n",
        "pretrained_model = torchvision.models.resnet34(pretrained=True)\n",
        "# Remove classification layer\n",
        "# In some models, it is called \"fc\", others have \"classifier\"\n",
        "# Setting both to an empty sequential represents an identity map of the final features.\n",
        "pretrained_model.fc = nn.Sequential()\n",
        "pretrained_model.classifier = nn.Sequential()\n",
        "# To GPU\n",
        "pretrained_model = pretrained_model.to(device)\n",
        "\n",
        "# Only eval, no gradient required\n",
        "pretrained_model.eval()\n",
        "for p in pretrained_model.parameters():\n",
        "    p.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4A_jzoMzPDn"
      },
      "source": [
        "We will now write a extraction function for the features below. This cell requires access to a GPU, as the model is rather deep and the images relatively large. The GPUs on GoogleColab are sufficient, but running this cell can take 2-3 minutes. Once it is run, the features are exported on disk so they don't have to be recalculated every time you run the notebook. However, this requires >150MB free disk space. So it is recommended to run this only on a local computer if you have enough free disk and a GPU (GoogleColab is fine for this). If you do not have a GPU, you can download the features from the [GoogleDrive folder](https://drive.google.com/drive/folders/1DF7POc6j03pRiWQPWSl5QJX5iY-xK0sV?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aI5lpbHqSW1Z"
      },
      "source": [
        "# reminder:\n",
        "# @torch.no_grad() -> https://pytorch.org/docs/stable/generated/torch.no_grad.html\n",
        "# Context-manager that disabled gradient calculation.\n",
        "\n",
        "# Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). \n",
        "# It will reduce memory consumption for computations that would otherwise have requires_grad=True.\n",
        "\n",
        "# each image is represented by vector of 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIiKMz8_bq9Q"
      },
      "source": [
        "\n",
        "CHECKPOINT_PATH1='/content/drive/My Drive/Colab_files/tutorial6'\n",
        "train_feat_file = os.path.join(CHECKPOINT_PATH1, \"train_set_features.tar\")\n",
        "print(train_feat_file, os.path.isfile(train_feat_file))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iaEHKrHzPDo"
      },
      "source": [
        "@torch.no_grad()\n",
        "def extract_features(dataset, save_file):\n",
        "    if not os.path.isfile(save_file):\n",
        "        data_loader = data.DataLoader(dataset, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
        "        extracted_features = []\n",
        "        for imgs, _ in tqdm(data_loader):\n",
        "            imgs = imgs.to(device)\n",
        "            feats = pretrained_model(imgs)\n",
        "            extracted_features.append(feats)\n",
        "        extracted_features = torch.cat(extracted_features, dim=0)\n",
        "        extracted_features = extracted_features.detach().cpu()\n",
        "        torch.save(extracted_features, save_file)\n",
        "    else:\n",
        "        extracted_features = torch.load(save_file)\n",
        "    return extracted_features\n",
        "\n",
        "CHECKPOINT_PATH1='/content/drive/My Drive/Colab_files/tutorial6'\n",
        "train_feat_file = os.path.join(CHECKPOINT_PATH1, \"train_set_features.tar\")\n",
        "train_set_feats = extract_features(train_set, train_feat_file)\n",
        "\n",
        "test_feat_file = os.path.join(CHECKPOINT_PATH1, \"test_set_features.tar\")\n",
        "test_feats = extract_features(test_set, test_feat_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUqIC1HJzPDo"
      },
      "source": [
        "Let's verify the feature shapes below. The training should have 50k elements, and the test 10k images. The feature dimension is 512 for the ResNet34. If you experiment with other models, you likely see a different feature dimension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UOKwYUOzPDo"
      },
      "source": [
        "print(\"Train:\", train_set_feats.shape)\n",
        "print(\"Test: \", test_feats.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2283puazPDo"
      },
      "source": [
        "As usual, we want to create a validation set to detect when we should stop training. In this case, we will split the training set into 90% training, 10% validation. However, the difficulty is here that we need to ensure that the validation set has the same number of images for all 100 labels. Otherwise, we have a class imbalance which is not good for creating the image sets. Hence, we take 10% of the images for each class, and move them into the validation set. The code below does exactly this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu9OCaUqzPDo"
      },
      "source": [
        "## Split train into train+val\n",
        "# Get labels from train set\n",
        "labels = train_set.targets\n",
        "\n",
        "# Get indices of images per class\n",
        "labels = torch.LongTensor(labels)\n",
        "num_labels = labels.max()+1\n",
        "sorted_indices = torch.argsort(labels).reshape(num_labels, -1) # [classes, num_imgs per class]\n",
        "\n",
        "# Determine number of validation images per class\n",
        "num_val_exmps = sorted_indices.shape[1] // 10\n",
        "\n",
        "# Get image indices for validation and training\n",
        "val_indices   = sorted_indices[:,:num_val_exmps].reshape(-1)\n",
        "train_indices = sorted_indices[:,num_val_exmps:].reshape(-1)\n",
        "\n",
        "# Group corresponding image features and labels\n",
        "train_feats, train_labels = train_set_feats[train_indices], labels[train_indices]\n",
        "val_feats,   val_labels   = train_set_feats[val_indices],   labels[val_indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEm1-sCZdaNL"
      },
      "source": [
        "print(train_labels.size(),train_feats.size())\n",
        "print(train_labels[-20:])\n",
        "print(np.random.randint(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN3mP31KzPDp"
      },
      "source": [
        "Now we can prepare a dataset class for the set anomaly task. We define an epoch to be the sequence in which each image has been exactly once as an \"anomaly\". Hence, the length of the dataset is the number of images in it. For the training set, each time we access an item with `__getitem__`, we sample a random, different class than the image at the corresponding index `idx` has. In a second step, we sample $N-1$ images of this sampled class. The set of 10 images is finally returned. The randomness in the `__getitem__` allows us to see a slightly different set during each iteration. However, we can't use the same strategy for the test set as we want the test dataset to be the same every time we iterate over it. Hence, we sample the sets in the `__init__` method, and return those in `__getitem__`. The code below implements exactly this dynamic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3JPUIurzPDp"
      },
      "source": [
        "class SetAnomalyDataset(data.Dataset):\n",
        "    \n",
        "    def __init__(self, img_feats, labels, set_size=10, train=True):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            img_feats - Tensor of shape [num_imgs, img_dim]. Represents the high-level features.\n",
        "            labels - Tensor of shape [num_imgs], containing the class labels for the images\n",
        "            set_size - Number of elements in a set. N-1 are sampled from one class, and one from another one.\n",
        "            train - If True, a new set will be sampled every time __getitem__ is called.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.img_feats = img_feats\n",
        "        self.labels = labels\n",
        "        self.set_size = set_size-1 # The set size is here the size of correct images\n",
        "        self.train = train\n",
        "        \n",
        "        # Tensors with indices of the images per class\n",
        "        self.num_labels = labels.max()+1\n",
        "        self.img_idx_by_label = torch.argsort(self.labels).reshape(self.num_labels, -1)\n",
        "        \n",
        "        if not train:\n",
        "            self.test_sets = self._create_test_sets()\n",
        "            \n",
        "            \n",
        "    def _create_test_sets(self):\n",
        "        # Pre-generates the sets for each image for the test set\n",
        "        test_sets = []\n",
        "        num_imgs = self.img_feats.shape[0]\n",
        "        np.random.seed(42)\n",
        "        test_sets = [self.sample_img_set(self.labels[idx]) for idx in range(num_imgs)]\n",
        "        test_sets = torch.stack(test_sets, dim=0)\n",
        "        return test_sets\n",
        "            \n",
        "        \n",
        "    def sample_img_set(self, anomaly_label):\n",
        "        \"\"\"\n",
        "        Samples a new set of images, given the label of the anomaly. \n",
        "        The sampled images come from a different class than anomaly_label\n",
        "        \"\"\"\n",
        "        # Sample class from 0,...,num_classes-1 while skipping anomaly_label as class\n",
        "        set_label = np.random.randint(self.num_labels-1)\n",
        "        if set_label >= anomaly_label:\n",
        "            set_label += 1\n",
        "            \n",
        "        # Sample images from the class determined above\n",
        "        img_indices = np.random.choice(self.img_idx_by_label.shape[1], size=self.set_size, replace=False)\n",
        "        img_indices = self.img_idx_by_label[set_label, img_indices]\n",
        "        return img_indices\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.img_feats.shape[0]\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        anomaly = self.img_feats[idx]\n",
        "        if self.train: # If train => sample\n",
        "            img_indices = self.sample_img_set(self.labels[idx])\n",
        "        else: # If test => use pre-generated ones\n",
        "            img_indices = self.test_sets[idx]\n",
        "            \n",
        "        # Concatenate images. The anomaly is always the last image for simplicity\n",
        "        img_set = torch.cat([self.img_feats[img_indices], anomaly[None]], dim=0)\n",
        "        indices = torch.cat([img_indices, torch.LongTensor([idx])], dim=0)\n",
        "        label = img_set.shape[0]-1\n",
        "        \n",
        "        # We return the indices of the images for visualization purpose. \"Label\" is the index of the anomaly\n",
        "        return img_set, indices, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjZl7aDjzPDp"
      },
      "source": [
        "Next, we can setup our datasets and data loaders below. Here, we will use a set size of 10, i.e. 9 images from one category + 1 anomaly. Feel free to change it if you want to experiment with the sizes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJDeBsHRzPDp"
      },
      "source": [
        "SET_SIZE = 10\n",
        "test_labels = torch.LongTensor(test_set.targets)\n",
        "\n",
        "train_anom_dataset = SetAnomalyDataset(train_feats, train_labels, set_size=SET_SIZE, train=True)\n",
        "val_anom_dataset   = SetAnomalyDataset(val_feats,   val_labels,   set_size=SET_SIZE, train=False)\n",
        "test_anom_dataset  = SetAnomalyDataset(test_feats,  test_labels,  set_size=SET_SIZE, train=False)\n",
        "\n",
        "train_anom_loader = data.DataLoader(train_anom_dataset, batch_size=64, shuffle=True,  drop_last=True,  num_workers=2, pin_memory=True)\n",
        "val_anom_loader   = data.DataLoader(val_anom_dataset,   batch_size=64, shuffle=False, drop_last=False, num_workers=2)\n",
        "test_anom_loader  = data.DataLoader(test_anom_dataset,  batch_size=64, shuffle=False, drop_last=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYkquwoBzPDq"
      },
      "source": [
        "To understand the dataset a little better, we can plot below a few sets from the test dataset. Each row shows a different input set, where the first 9 are from the same class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6zJ3tw8zPDq"
      },
      "source": [
        "def visualize_exmp(indices, orig_dataset):\n",
        "    images = [orig_dataset[idx][0] for idx in indices.reshape(-1)]\n",
        "    images = torch.stack(images, dim=0)\n",
        "    images = images * TORCH_DATA_STD + TORCH_DATA_MEANS\n",
        "    \n",
        "    img_grid = torchvision.utils.make_grid(images, nrow=SET_SIZE, normalize=True, pad_value=0.5, padding=16)\n",
        "    img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "    plt.figure(figsize=(12,8))\n",
        "    plt.title(\"Anomaly examples on CIFAR100\")\n",
        "    plt.imshow(img_grid)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "_, indices, _ = next(iter(test_anom_loader))\n",
        "visualize_exmp(indices[:4], test_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIdNJUC-zPDq"
      },
      "source": [
        "We can already see that for some sets the task might be easier than for others. Difficulties can especially arise if the anomaly is in a different, but yet visually similar class (e.g. train vs bus, flour vs worm, etc.).\n",
        "\n",
        "After having prepared the data, we can look closer at the model. Here, we have a classification of the whole set. For the prediction to be permutation-equivariant, we will output one logit for each image. Over these logits, we apply a softmax and train the anomaly image to have the highest score/probability. This is a bit different than a standard classification layer as the softmax is applied over images, not over output classes in the classical sense. However, if we swap two images in their position, we effectively swap their position in the output softmax. Hence, the prediction is equivariant with respect to the input. We implement this idea below in the subclass of the Transformer Lightning module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGPlIA8szPDq"
      },
      "source": [
        "class AnomalyPredictor(TransformerPredictor):\n",
        "    \n",
        "    def _calculate_loss(self, batch, mode=\"train\"):\n",
        "        img_sets, _, labels = batch\n",
        "        preds = self.forward(img_sets, add_positional_encoding=False) # No positional encodings as it is a set, not a sequence!\n",
        "        preds = preds.squeeze(dim=-1) # Shape: [Batch_size, set_size]\n",
        "        loss = F.cross_entropy(preds, labels) # Softmax/CE over set dimension\n",
        "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
        "        self.log(\"%s_loss\" % mode, loss)\n",
        "        self.log(\"%s_acc\" % mode, acc, on_step=False, on_epoch=True)\n",
        "        return loss, acc\n",
        "        \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, _ = self._calculate_loss(batch, mode=\"train\")\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        _ = self._calculate_loss(batch, mode=\"val\")\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        _ = self._calculate_loss(batch, mode=\"test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLCIrvxezPDr"
      },
      "source": [
        "Finally, we write our train function below. It has the exact same structure as the reverse task one, hence not much of an explanation is needed here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjwb8kPlzPDr"
      },
      "source": [
        "def train_anomaly(**kwargs):\n",
        "    # Create a PyTorch Lightning trainer with the generation callback\n",
        "    root_dir = os.path.join(CHECKPOINT_PATH, \"SetAnomalyTask\")\n",
        "    os.makedirs(root_dir, exist_ok=True)\n",
        "    trainer = pl.Trainer(default_root_dir=root_dir, \n",
        "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
        "                         gpus=1 if str(device).startswith(\"cuda\") else 0, \n",
        "                         max_epochs=100,\n",
        "                         gradient_clip_val=2,\n",
        "                         progress_bar_refresh_rate=1)\n",
        "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
        "    \n",
        "    # Check whether pretrained model exists. If yes, load it and skip training\n",
        "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"SetAnomalyTask.ckpt\")\n",
        "    if os.path.isfile(pretrained_filename):\n",
        "        print(\"Found pretrained model, loading...\")\n",
        "        model = AnomalyPredictor.load_from_checkpoint(pretrained_filename)\n",
        "    else:\n",
        "        model = AnomalyPredictor(max_iters=trainer.max_epochs*len(train_anom_loader), **kwargs)\n",
        "        trainer.fit(model, train_anom_loader, val_anom_loader)\n",
        "        model = AnomalyPredictor.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
        "    \n",
        "    # Test best model on validation and test set\n",
        "    train_result = trainer.test(model, test_dataloaders=train_anom_loader, verbose=False)\n",
        "    val_result = trainer.test(model, test_dataloaders=val_anom_loader, verbose=True)\n",
        "    print(val_result)\n",
        "    test_result = trainer.test(model, test_dataloaders=test_anom_loader, verbose=False)\n",
        "    result = {\"test_acc\": test_result[0][\"test_acc\"], \"val_acc\": val_result[0][\"test_acc\"], \"train_acc\": train_result[0][\"test_acc\"]}\n",
        "    \n",
        "    model = model.to(device)\n",
        "    return model, result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEpZ2_sqzPDr"
      },
      "source": [
        "Let's finally train our model. We will use 4 layers with 4 attention heads each. The hidden dimensionality of the model is 256, and we use a dropout of 0.1 throughout the model for good regularization. Note that we also apply the dropout on the input features, as this makes the model more robust against image noise and generalizes better. Again, we use warmup to slowly start our model training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--EHsPeMzPDr"
      },
      "source": [
        "anomaly_model, anomaly_result = train_anomaly(input_dim=train_anom_dataset.img_feats.shape[-1],\n",
        "                                              model_dim=256,\n",
        "                                              num_heads=4,\n",
        "                                              num_classes=1,\n",
        "                                              num_layers=4,\n",
        "                                              dropout=0.1,\n",
        "                                              input_dropout=0.1,\n",
        "                                              lr=5e-4,\n",
        "                                              warmup=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gffy-PozPDr"
      },
      "source": [
        "We can print the achieved accuracy below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAChBm9qzPDs"
      },
      "source": [
        "print(\"Train accuracy: %4.2f%%\" % (100.0*anomaly_result[\"train_acc\"]))\n",
        "print(\"Val accuracy:   %4.2f%%\" % (100.0*anomaly_result[\"val_acc\"]))\n",
        "print(\"Test accuracy:  %4.2f%%\" % (100.0*anomaly_result[\"test_acc\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V5R5Bb8zPDs"
      },
      "source": [
        "With ~94% validation and test accuracy, the model generalizes quite well. It should be noted that you might see slightly different scores depending on what computer/device you are running this notebook. This is because despite setting the seed before generating the test dataset, it is not the same across platforms and numpy versions. Nevertheless, we can conclude that the model performs quite well and can solve the task for most sets. Before trying to interpret the model, let's verify that our model is permutation-equivariant, and assigns the same predictions for different permutations of the input set. For this, we sample a batch from the test set and run it through the model to obtain the probabilities. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnyVbjwNzPDs"
      },
      "source": [
        "inp_data, indices, labels = next(iter(test_anom_loader))\n",
        "inp_data = inp_data.to(device)\n",
        "\n",
        "anomaly_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds = anomaly_model.forward(inp_data, add_positional_encoding=False)\n",
        "    preds = F.softmax(preds.squeeze(dim=-1), dim=-1)\n",
        "\n",
        "    # Permut input data\n",
        "    permut = np.random.permutation(inp_data.shape[1])\n",
        "    perm_inp_data = inp_data[:,permut]\n",
        "    perm_preds = anomaly_model.forward(perm_inp_data, add_positional_encoding=False)\n",
        "    perm_preds = F.softmax(perm_preds.squeeze(dim=-1), dim=-1)\n",
        "\n",
        "assert (preds[:,permut] - perm_preds).abs().max() < 1e-5, \"Predictions are not permutation equivariant\"\n",
        "\n",
        "print(\"Preds\\n\", preds[0,permut].cpu().numpy())\n",
        "print(\"Permuted preds\\n\", perm_preds[0].cpu().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIuy5NclzPDs"
      },
      "source": [
        "You can see that the predictions are almost exactly the same, and only differ because of slight numerical differences inside the network operation.\n",
        "\n",
        "To interpret the model a little more, we can plot the attention maps inside the model. This will give us an idea of what information the model is sharing/communicating between images, and what each head might represent. First, we need to extract the attention maps for the test batch above, and determine the discrete predictions for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp_6q-VUzPDs"
      },
      "source": [
        "attention_maps = anomaly_model.get_attention_maps(inp_data, add_positional_encoding=False)\n",
        "predictions = preds.argmax(dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g9SjGTFzPDt"
      },
      "source": [
        "Below we write a plot function which plots the images in the input set, the prediction of the model, and the attention maps of the different heads on layers of the transformer. Feel free to explore the attention maps for different input examples as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjDm13sVzPDt"
      },
      "source": [
        "def visualize_prediction(idx):\n",
        "    visualize_exmp(indices[idx:idx+1], test_set)\n",
        "    print(\"Prediction:\", predictions[idx].item())\n",
        "    plot_attention_maps(input_data=None, attn_maps=attention_maps, idx=idx)\n",
        "\n",
        "visualize_prediction(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAshMZnszPDt"
      },
      "source": [
        "Depending on the random seed, you might see a slightly different input set. For the version on the website, we compare 9 tree images with a volcano. We see that multiple heads, for instance, Layer 2 Head 1, Layer 2 Head 3, and Layer 3 Head 1 focus on the last image. Additionally, the heads in Layer 4 all seem to ignore the last image and assign a very low attention probability to it. This shows that the model has indeed recognized that the image doesn't fit the setting, and hence predicted it to be the anomaly. Layer 3 Head 2-4 seems to take a slightly weighted average of all images. That might indicate that the model extracts the \"average\" information of all images, to compare it to the image features itself. \n",
        "\n",
        "Let's try to find where the model actually makes a mistake. We can do this by identifying the sets where the model predicts something else than 9, as in the dataset, we ensured that the anomaly is always at the last position in the set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_JOc-B2zPDt"
      },
      "source": [
        "mistakes = torch.where(predictions != 9)[0].cpu().numpy()\n",
        "print(\"Indices with mistake:\", mistakes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNlkQ3xPzPDu"
      },
      "source": [
        "As our model achieves ~94% accuracy, we only have very little number of mistakes in a batch of 64 sets. Still, let's visualize one of them, for example the last one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-l1fUGxzPDu"
      },
      "source": [
        "visualize_prediction(mistakes[-1])\n",
        "print(\"Probabilities:\")\n",
        "for i, p in enumerate(preds[mistakes[-1]].cpu().numpy()):\n",
        "    print(\"Image %i: %4.2f%%\" % (i, 100.0*p))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKzku_EAzPDu"
      },
      "source": [
        "In this example, the model confuses a palm tree with a building, giving a probability of ~90% to image 2, and 8% to the actual anomaly. However, the difficulty here is that the picture of the building has been taken at a similar angle as the palms. Meanwhile, image 2 shows a rather unusual palm with a different color palette, which is why the model fails here. Nevertheless, in general, the model performs quite well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOFMx-BlzPDu"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, we took a closer look at the Multi-Head Attention layer which uses a scaled dot product between queries and keys to find correlations and similarities between input elements. The Transformer architecture is based on the Multi-Head Attention layer and applies multiple of them in a ResNet-like block. The Transformer is a very important, recent architecture that can be applied to many tasks and datasets. Although it is best known for its success in NLP, there is so much more to it. We have seen its application on sequence-to-sequence tasks and set anomaly detection. Its property of being permutation-equivariant if we do not provide any positional encodings, allows it to generalize to many settings. Hence, it is important to know the architecture, but also its possible issues such as the gradient problem during the first iterations solved by learning rate warm-up. If you are interested in continuing with the study of the Transformer architecture, please have a look at the blog posts listed at the beginning of the tutorial notebook."
      ]
    }
  ]
}